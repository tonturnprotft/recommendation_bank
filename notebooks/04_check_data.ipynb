{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2eb9f744",
   "metadata": {},
   "source": [
    "\n",
    "# 04 — Sanity Checks & Quick Stats\n",
    "\n",
    "Verifies:\n",
    "- Balanced IDs exist and overlap with JSONLs.\n",
    "- Sample records preview.\n",
    "- Class distribution in raw (derived) targets and in the balanced subset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dbaeef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Presence check ===\n",
      "[OK] /Users/tree/Projects/recommemdation_bank/outputs/balanced/mbd_targets_balanced.parquet\n",
      "[OK] /Users/tree/Projects/recommemdation_bank/outputs/balanced/targets_raw.parquet\n",
      "[OK] /Users/tree/Projects/recommemdation_bank/outputs/json/trx/mbd_all.jsonl\n",
      "[OK] /Users/tree/Projects/recommemdation_bank/outputs/json/trx/json_balanced_trx.jsonl\n",
      "[OK] /Users/tree/Projects/recommemdation_bank/outputs/json/geo/mbd_all.jsonl\n",
      "[OK] /Users/tree/Projects/recommemdation_bank/outputs/json/geo/json_balanced_geo.jsonl\n",
      "[OK] /Users/tree/Projects/recommemdation_bank/outputs/json/mm/mbd_all.jsonl\n",
      "[OK] /Users/tree/Projects/recommemdation_bank/outputs/json/mm/json_balanced_mm.jsonl\n",
      "\n",
      "=== Balanced IDs & Targets ===\n",
      "Balanced clients: 2132\n",
      "Targets shape: (100224, 6) | target cols: ['target_1', 'target_2', 'target_3', 'target_4']\n",
      "\n",
      "=== JSONL ID coverage ===\n",
      "#TRX all: 98721 | balanced: 2118\n",
      "#GEO all: 72573 | balanced: 1623\n",
      "#MM  all: 99647 | balanced: 2127\n",
      "\n",
      "=== Union(TRX,GEO) vs MM (balanced) ===\n",
      "Union(TRX,GEO) balanced: 2127\n",
      "MM covers union(TRX,GEO)? True\n",
      "Missing from MM (should be 0): 0\n",
      "\n",
      "=== Balanced clients with no TRX & no GEO ===\n",
      "Count: 5\n",
      "Sample: ['36dcfb0b8e9c249174f7147895488d5e65505eb8c976f090fd367f4ec8aedafb', '76fa4cc1a08e640b21e41c2c7d21471b93af1ec2d152ea2473c682b6098209ac', 'cd82629a251ce8831b06b0d946a5e67e53cc1c61a5c7086bbc945aa564b50010', 'ea62be7cbfe00fdba7bf89919574a61a96fcac02cade60a0e07c4ebd75b4da08', 'ed435f029b3cc3b3160ef5eb85375de5b45f9906ef4704248849f49746cf1753']\n",
      "\n",
      "=== Fold membership for clients with no TRX/GEO ===\n",
      "fold\n",
      "-1    5\n",
      "\n",
      "=== Leakage checks (client in multiple folds) ===\n",
      "TRX clients in >1 fold: 0\n",
      "GEO clients in >1 fold: 0\n",
      "\n",
      "=== Class distributions (raw targets vs balanced subset) ===\n",
      "Raw targets:\n",
      "  target_1: {0: 99667, 1: 557}\n",
      "  target_2: {0: 100191, 1: 33}\n",
      "  target_3: {0: 99948, 1: 276}\n",
      "  target_4: {0: 99986, 1: 238}\n",
      "Balanced subset:\n",
      "  target_1: {0: 1575, 1: 557}\n",
      "  target_2: {0: 2099, 1: 33}\n",
      "  target_3: {0: 1856, 1: 276}\n",
      "  target_4: {0: 1894, 1: 238}\n",
      "\n",
      "=== Text length stats (balanced JSONLs) ===\n",
      "TRX: {'count': 2118, 'mean': 726.1406987724268, 'p50': 1026.0, 'p90': 1026.0, 'p99': 1026.0, 'max': 1026}\n",
      "GEO: {'count': 1623, 'mean': 231.42698706099816, 'p50': 258.0, 'p90': 258.0, 'p99': 258.0, 'max': 258}\n",
      "MM : {'count': 2127, 'mean': 900.1401034320639, 'p50': 1028.0, 'p90': 1284.0, 'p99': 1284.0, 'max': 1284}\n",
      "\n",
      "=== Sample balanced records (trimmed) ===\n",
      "TRX samples: [{'client_id': 'b58829e5d540fb87b8c2c51ed821709bc1ce9df8d37979f8f9bf4fb5f4bd2771', 'text': '<TRX> 2022-09-19 t14 a6.02 s73 2022-09-20 t46 a5.12 s73 2022-09-21 t40 a1.81 s73 2022-09-22 t1 a3.43 s73 2022-09-22 t14 a6.24 s73 2022-09-23 t1 a4.87 s73 2022-09-24 t1 a3.57 s73 2022-09-24 t49 a3.68 s73 2022-09-24 t14 a6.29 s73 2022-09-25 t49 a3.62 s73 2022-09-25 t49 a2.89 s73 2022-09-25 t1 a2.94 s73 2022-09-26 t1 a5.33 s73 2022-09-26 t1 a5.11 s73 2022-09-28 t40 a1.24 s73 2022-09-28 t49 a4.26 s73 2022-09-29 t1 a3.52 s73 2022-09-30 t11 a4.46 s73 2022-10-02 t49 a3.86 s73 2022-10-02 t1 a3.26 s73 2022-10-02 t40 a3.31 s73 2022-10-02 t1 a3.33 s73 2022-10-02 t11 a3.11 s73 2022-10-03 t1 a4.79 s73 2022-10-03 t1 a4.31 s73 2022-10-04 t40 a2.20 s73 2022-10-04 t1 a3.15 s73 2022-10-04 t1 a4.16 s73 2022-10-05 t1 a4.49 s73 2022-10-05 t1 a4.85 s73 2022-10-05 t11 a5.47 s73 2022-10-05 t1 a5.85 s73 2022-10-05'}, {'client_id': '1612d1f9d90304bf3cc127d5f3dae8a1f18c751a89eaaf3a818a8cf734311c94', 'text': '<TRX> 2021-01-18 t39 a1.92 s16 2021-01-27 t41 a1.99 s16 2021-01-31 t18 a2.62 s16 2021-01-31 t18 a1.51 s16 2021-02-10 t40 a1.91 s16 2021-03-01 t40 a2.12 s16 2021-03-09 t41 a2.44 s16 2021-03-10 t39 a-0.06 s16 2021-03-20 t40 a1.74 s16 2021-03-27 t40 a1.86 s16 2021-04-29 t40 a2.13 s16 2021-05-04 t27 a4.34 s16 2021-05-05 t27 a2.19 s16 2021-05-13 t40 a2.65 s16 2021-05-23 t27 a3.78 s16 2021-06-07 t40 a2.93 s16 2021-06-17 t27 a2.54 s16 2021-06-23 t41 a2.60 s16 2021-06-24 t18 a3.17 s16 2021-06-25 t39 a1.16 s16 2021-06-27 t41 a3.12 s16 2021-07-02 t18 a2.60 s16 2021-07-13 t18 a2.07 s16 2021-07-19 t41 a2.41 s16 2021-07-20 t41 a2.93 s16 2021-08-05 t39 a2.52 s16 2021-08-06 t18 a2.48 s16 2021-08-06 t39 a2.09 s16 2021-08-09 t39 a3.17 s16 2021-08-10 t18 a3.13 s16 2021-08-15 t41 a2.86 s16 2021-08-22 t41 a2.'}]\n",
      "GEO samples: [{'client_id': 'f34813be582384604568dda25ba7b1f5ac41fba4b2de83ab55dccf8981f07942', 'text': '<GEO> 2022-09-13 g41982 g5268454 g62333943 2022-09-14 g417197 g5366886 g62283850 2022-09-15 g41982 g5268454 g62333943 2022-09-18 g41982 g592111 g6145583 2022-09-22 g41982 g5268454 g62412411 2022-09-25 g417197 g5366886 g62283850 2022-09-26 g41982 g5268454 g62412411 2022-10-02 g448842 g5347169 g62120476 2022-10-10 g417197 g5366886 g61647845 2022-10-12 g41982 g5380396 g61558294 2022-10-14 g417197 g5366886 g61748380 2022-10-15 g41982 g5268454 g62412411 2022-10-16 g41982 g5268454 g62333943 2022-10-17 g417197 g5366886 g62283850 2022-10-17 g448842 g5347169 g62120476 2022-10-17 g41982 g5268454 g62412411 2022-10-18 g417197 g5366886 g62283850 2022-10-23 g448842 g5347169 g62120476 2022-10-23 g41982 g5268454 g62333943 2022-10-24 g448842 g5347169 g62120476 2022-10-26 g417197 g5366886 g61748380 2022-10-'}, {'client_id': 'e2f1d85ffce5319e01abb74b2f84adb27b6c39c34fc512be017aeb30ade68b40', 'text': '<GEO> 2022-02-16 g410608 g5169547 g6838638 2022-02-18 g410608 g5230190 g6637439 2022-02-19 g410608 g5169547 g6838638 2022-02-19 g410608 g5377340 g6690149 2022-02-20 g410608 g5169547 g6838638 2022-02-26 g410608 g5230190 g6637439 2022-02-26 g410608 g5169547 g6838638 2022-02-28 g410608 g5169547 g668003 2022-03-02 g410608 g5169547 g6838638 2022-03-08 g410608 g5169547 g61032599 2022-03-10 g410608 g5169547 g6838638 2022-03-12 g410608 g5377340 g668480 2022-03-12 g410608 g5169547 g6838638 2022-03-14 g410608 g5169547 g61400469 2022-03-14 g410608 g5169547 g6838638 2022-03-24 g410608 g5230190 g6703036 2022-03-25 g410608 g5169547 g6838638 2022-03-30 g410608 g5169547 g62829516 2022-03-30 g410608 g5169547 g6838638 2022-04-18 g410608 g5118851 g62225052 2022-04-19 g410608 g5169547 g6838638 2022-05-03 g410'}]\n",
      "MM  samples: [{'client_id': '0ba9098b6117bf9ad603e34a3b895a5a715b46b76c71c0d73f1675da7ac4bb6b', 'text': '<TRX> 2022-04-13 t11 a5.44 s73 2022-04-16 t46 a5.06 s73 2022-04-16 t40 a1.99 s73 2022-04-17 t11 a4.13 s73 2022-04-17 t11 a3.76 s73 2022-04-18 t40 a1.51 s73 2022-04-20 t11 a3.03 s73 2022-04-21 t50 a4.23 s73 2022-04-21 t36 a4.11 s73 2022-04-21 t11 a3.86 s73 2022-04-21 t40 a2.39 s73 2022-04-22 t1 a4.46 s73 2022-04-23 t50 a4.57 s73 2022-04-23 t40 a1.77 s73 2022-04-25 t40 a2.27 s73 2022-04-26 t11 a2.99 s73 2022-04-27 t40 a2.63 s73 2022-04-29 t40 a1.79 s73 2022-04-29 t40 a2.26 s73 2022-04-30 t11 a4.95 s73 2022-05-02 t46 a4.68 s73 2022-05-02 t36 a2.47 s73 2022-05-04 t11 a3.93 s73 2022-05-04 t40 a3.08 s73 2022-05-05 t4 a5.03 s73 2022-05-06 t40 a1.40 s73 2022-05-07 t40 a3.25 s73 2022-05-08 t46 a4.98 s73 2022-05-09 t1 a4.69 s73 2022-05-10 t55 a4.91 s73 2022-05-13 t40 a1.22 s73 2022-05-14 t40 a4.20 s'}, {'client_id': 'c2a4a31213df3f1ad659cfb1a5ec0710bfa0dec1bd3c898826c71b9861d3d690', 'text': '<TRX> 2022-01-23 t40 a1.13 s36 2022-01-28 t1 a2.67 s36 2022-02-05 t40 a2.79 s36 2022-02-19 t49 a3.86 s36 2022-02-20 t1 a2.50 s36 2022-02-21 t40 a3.02 s36 2022-02-21 t40 a3.52 s36 2022-02-22 t49 a3.32 s36 2022-02-23 t49 a4.00 s36 2022-02-23 t49 a2.40 s36 2022-02-25 t40 a2.50 s36 2022-02-26 t49 a3.34 s36 2022-02-26 t49 a2.86 s36 2022-02-27 t1 a3.31 s36 2022-02-27 t56 a5.07 s36 2022-03-03 t40 a2.02 s36 2022-03-04 t49 a3.62 s36 2022-03-05 t49 a4.92 s36 2022-03-06 t49 a2.71 s36 2022-03-06 t49 a4.01 s36 2022-03-07 t49 a3.53 s36 2022-03-08 t49 a3.03 s36 2022-03-08 t40 a2.83 s36 2022-03-08 t49 a2.67 s36 2022-03-08 t49 a3.05 s36 2022-03-08 t49 a1.83 s36 2022-03-08 t49 a3.05 s36 2022-03-10 t40 a2.51 s36 2022-03-12 t40 a2.04 s36 2022-03-13 t40 a2.50 s36 2022-03-15 t49 a4.92 s36 2022-03-16 t40 a2.59 s'}]\n"
     ]
    }
   ],
   "source": [
    "# 04_check_data.py — robust sanity + coverage + leakage checks\n",
    "\n",
    "import os, glob, json, re, random\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ====== CONFIG (keep in sync with 00/01/02/03) ======\n",
    "BASE_OUT = \"/Users/tree/Projects/recommemdation_bank/outputs\"\n",
    "\n",
    "BAL_DIR            = f\"{BASE_OUT}/balanced\"\n",
    "BALANCED_PATH      = f\"{BAL_DIR}/mbd_targets_balanced.parquet\"\n",
    "TARGETS_RAW_PATH   = f\"{BAL_DIR}/targets_raw.parquet\"\n",
    "\n",
    "JSON_TRX_ALL       = f\"{BASE_OUT}/json/trx/mbd_all.jsonl\"\n",
    "JSON_GEO_ALL       = f\"{BASE_OUT}/json/geo/mbd_all.jsonl\"\n",
    "JSON_MM_ALL        = f\"{BASE_OUT}/json/mm/mbd_all.jsonl\"\n",
    "\n",
    "JSON_TRX_BAL       = f\"{BASE_OUT}/json/trx/json_balanced_trx.jsonl\"\n",
    "JSON_GEO_BAL       = f\"{BASE_OUT}/json/geo/json_balanced_geo.jsonl\"\n",
    "JSON_MM_BAL        = f\"{BASE_OUT}/json/mm/json_balanced_mm.jsonl\"\n",
    "\n",
    "TRX_GLOB           = \"/Users/tree/Projects/recommemdation_bank/data/mbd_mini/detail/trx/fold=*/part-*.parquet\"\n",
    "GEO_GLOB           = \"/Users/tree/Projects/recommemdation_bank/data/mbd_mini/detail/geo/fold=*/part-*.parquet\"\n",
    "\n",
    "FOLDS              = [0,1,2,3,4]\n",
    "\n",
    "def _exists(p):\n",
    "    ok = os.path.exists(p)\n",
    "    print(f\"[{'OK' if ok else 'MISS'}] {p}\")\n",
    "    return ok\n",
    "\n",
    "def _load_jsonl_ids(path):\n",
    "    \"\"\"Load unique client_ids from JSONL; return empty set if missing.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"[WARN] JSONL missing: {path}\")\n",
    "        return set()\n",
    "    ids=set()\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                rec=json.loads(line)\n",
    "                cid=str(rec.get(\"client_id\",\"\"))\n",
    "                if cid:\n",
    "                    ids.add(cid)\n",
    "            except Exception:\n",
    "                continue\n",
    "    return ids\n",
    "\n",
    "def _count_jsonl(path):\n",
    "    if not os.path.exists(path):\n",
    "        return 0\n",
    "    n=0\n",
    "    with open(path,\"r\") as f:\n",
    "        for _ in f: n+=1\n",
    "    return n\n",
    "\n",
    "def _read_with_inferred_fold(glob_pattern, need_cols=(\"client_id\",\"fold\")):\n",
    "    \"\"\"Read parquet parts; infer 'fold' from path if missing. Returns empty DF if no files.\"\"\"\n",
    "    paths = sorted(glob.glob(glob_pattern))\n",
    "    if not paths:\n",
    "        print(f\"[WARN] No files matched: {glob_pattern}\")\n",
    "        return pd.DataFrame(columns=list(need_cols))\n",
    "    dfs=[]\n",
    "    for p in paths:\n",
    "        try:\n",
    "            cols_req = [c for c in need_cols if c != \"fold\"]\n",
    "            dfp = pd.read_parquet(p, columns=cols_req if cols_req else None)\n",
    "        except Exception:\n",
    "            dfp = pd.read_parquet(p)\n",
    "            dfp = dfp[[c for c in need_cols if c in dfp.columns]]\n",
    "        if \"fold\" in need_cols and \"fold\" not in dfp.columns:\n",
    "            m = re.search(r\"fold=(\\d+)\", p)\n",
    "            dfp[\"fold\"] = int(m.group(1)) if m else -1\n",
    "        if \"client_id\" in dfp.columns:\n",
    "            dfp[\"client_id\"] = dfp[\"client_id\"].astype(str)\n",
    "        dfs.append(dfp)\n",
    "    if not dfs:\n",
    "        return pd.DataFrame(columns=list(need_cols))\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "def _text_length_stats(path, sample=5000):\n",
    "    \"\"\"Approx word counts; safe if file missing.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        return {\"count\": 0}\n",
    "    L=[]\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "    if not lines:\n",
    "        return {\"count\": 0}\n",
    "    for line in random.sample(lines, min(sample, len(lines))):\n",
    "        try:\n",
    "            t = json.loads(line).get(\"text\",\"\")\n",
    "        except Exception:\n",
    "            t = \"\"\n",
    "        L.append(len(str(t).split()))\n",
    "    if not L:\n",
    "        return {\"count\": 0}\n",
    "    a=np.array(L)\n",
    "    return {\n",
    "        \"count\": len(L),\n",
    "        \"mean\": float(np.mean(a)),\n",
    "        \"p50\":  float(np.percentile(a,50)),\n",
    "        \"p90\":  float(np.percentile(a,90)),\n",
    "        \"p99\":  float(np.percentile(a,99)),\n",
    "        \"max\":  int(np.max(a)),\n",
    "    }\n",
    "\n",
    "def _sample_json(path, k=2, trim=800):\n",
    "    \"\"\"Return k example records (trimmed) from a JSONL, or [] if missing.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        return []\n",
    "    with open(path,\"r\") as f:\n",
    "        lines=f.readlines()\n",
    "    out=[]\n",
    "    for line in random.sample(lines, min(k,len(lines))):\n",
    "        try:\n",
    "            rec=json.loads(line)\n",
    "        except Exception:\n",
    "            continue\n",
    "        rec2 = {\n",
    "            \"client_id\": rec.get(\"client_id\"),\n",
    "            \"text\": str(rec.get(\"text\",\"\")).replace(\"\\n\",\" \")[:trim]\n",
    "        }\n",
    "        out.append(rec2)\n",
    "    return out\n",
    "\n",
    "# ====== 0) Show what's present ======\n",
    "print(\"=== Presence check ===\")\n",
    "_ = [_exists(p) for p in [\n",
    "    BALANCED_PATH, TARGETS_RAW_PATH,\n",
    "    JSON_TRX_ALL, JSON_TRX_BAL,\n",
    "    JSON_GEO_ALL, JSON_GEO_BAL,\n",
    "    JSON_MM_ALL,  JSON_MM_BAL\n",
    "]]\n",
    "\n",
    "# ====== 1) Balanced list & targets ======\n",
    "print(\"\\n=== Balanced IDs & Targets ===\")\n",
    "if os.path.exists(BALANCED_PATH):\n",
    "    bal = pd.read_parquet(BALANCED_PATH)\n",
    "    bal_ids = set(bal[\"client_id\"].astype(str))\n",
    "    print(\"Balanced clients:\", len(bal_ids))\n",
    "else:\n",
    "    bal_ids = set()\n",
    "    print(\"[WARN] Balanced parquet not found.\")\n",
    "\n",
    "if os.path.exists(TARGETS_RAW_PATH):\n",
    "    targets = pd.read_parquet(TARGETS_RAW_PATH)\n",
    "    targets[\"client_id\"] = targets[\"client_id\"].astype(str)\n",
    "    tcols = [c for c in targets.columns if c.startswith(\"target_\")]\n",
    "    print(\"Targets shape:\", targets.shape, \"| target cols:\", tcols)\n",
    "else:\n",
    "    targets = pd.DataFrame()\n",
    "    tcols = []\n",
    "    print(\"[WARN] targets_raw parquet not found.\")\n",
    "\n",
    "# ====== 2) Load ids from JSONLs ======\n",
    "print(\"\\n=== JSONL ID coverage ===\")\n",
    "ids_trx_all = _load_jsonl_ids(JSON_TRX_ALL)\n",
    "ids_geo_all = _load_jsonl_ids(JSON_GEO_ALL)\n",
    "ids_mm_all  = _load_jsonl_ids(JSON_MM_ALL)\n",
    "\n",
    "ids_trx_bal = _load_jsonl_ids(JSON_TRX_BAL)\n",
    "ids_geo_bal = _load_jsonl_ids(JSON_GEO_BAL)\n",
    "ids_mm_bal  = _load_jsonl_ids(JSON_MM_BAL)\n",
    "\n",
    "print(f\"#TRX all: {len(ids_trx_all)} | balanced: {len(ids_trx_bal)}\")\n",
    "print(f\"#GEO all: {len(ids_geo_all)} | balanced: {len(ids_geo_bal)}\")\n",
    "print(f\"#MM  all: {len(ids_mm_all)} | balanced: {len(ids_mm_bal)}\")\n",
    "\n",
    "# ====== 3) Coverage: Union(TRX,GEO) vs MM ======\n",
    "print(\"\\n=== Union(TRX,GEO) vs MM (balanced) ===\")\n",
    "union_bal = ids_trx_bal | ids_geo_bal\n",
    "print(\"Union(TRX,GEO) balanced:\", len(union_bal))\n",
    "if ids_mm_bal:\n",
    "    print(\"MM covers union(TRX,GEO)?\", union_bal <= ids_mm_bal)\n",
    "    miss_in_mm = union_bal - ids_mm_bal\n",
    "    print(\"Missing from MM (should be 0):\", len(miss_in_mm))\n",
    "else:\n",
    "    print(\"[WARN] MM balanced JSON missing or empty; cannot compare.\")\n",
    "    miss_in_mm = set()\n",
    "\n",
    "# ====== 4) Balanced clients with no TRX & no GEO ======\n",
    "print(\"\\n=== Balanced clients with no TRX & no GEO ===\")\n",
    "missing_any_modality = bal_ids - union_bal if bal_ids else set()\n",
    "print(\"Count:\", len(missing_any_modality))\n",
    "peek = sorted(list(missing_any_modality))[:20]\n",
    "if peek:\n",
    "    print(\"Sample:\", peek)\n",
    "\n",
    "# ====== 5) Fold membership for clients with no TRX/GEO ======\n",
    "print(\"\\n=== Fold membership for clients with no TRX/GEO ===\")\n",
    "if not targets.empty and missing_any_modality:\n",
    "    fold_counts = (targets[targets[\"client_id\"].isin(missing_any_modality)][\"fold\"]\n",
    "                   .value_counts().sort_index())\n",
    "    print(fold_counts.to_string())\n",
    "else:\n",
    "    print(\"(skip) No targets or no missing clients.\")\n",
    "\n",
    "# ====== 6) Leakage: same client in multiple folds ======\n",
    "print(\"\\n=== Leakage checks (client in multiple folds) ===\")\n",
    "trx_ids_df = _read_with_inferred_fold(TRX_GLOB, need_cols=(\"client_id\",\"fold\"))\n",
    "geo_ids_df = _read_with_inferred_fold(GEO_GLOB, need_cols=(\"client_id\",\"fold\"))\n",
    "\n",
    "if not trx_ids_df.empty:\n",
    "    num_trx_multi = (trx_ids_df.groupby(\"client_id\")[\"fold\"].nunique() > 1).sum()\n",
    "    print(\"TRX clients in >1 fold:\", int(num_trx_multi))\n",
    "    if num_trx_multi > 0:\n",
    "        offenders = (trx_ids_df.groupby(\"client_id\")[\"fold\"].nunique() > 1)\n",
    "        print(\"Sample TRX offenders:\", offenders[offenders].index.tolist()[:20])\n",
    "else:\n",
    "    print(\"[WARN] TRX source empty; skip leakage check.\")\n",
    "\n",
    "if not geo_ids_df.empty:\n",
    "    num_geo_multi = (geo_ids_df.groupby(\"client_id\")[\"fold\"].nunique() > 1).sum()\n",
    "    print(\"GEO clients in >1 fold:\", int(num_geo_multi))\n",
    "    if num_geo_multi > 0:\n",
    "        offenders = (geo_ids_df.groupby(\"client_id\")[\"fold\"].nunique() > 1)\n",
    "        print(\"Sample GEO offenders:\", offenders[offenders].index.tolist()[:20])\n",
    "else:\n",
    "    print(\"[WARN] GEO source empty; skip leakage check.\")\n",
    "\n",
    "# ====== 7) Class distribution (raw + balanced subset) ======\n",
    "print(\"\\n=== Class distributions (raw targets vs balanced subset) ===\")\n",
    "if not targets.empty and tcols:\n",
    "    print(\"Raw targets:\")\n",
    "    for t in tcols:\n",
    "        vc = targets[t].value_counts().to_dict()\n",
    "        print(f\"  {t}: {vc}\")\n",
    "    if bal_ids:\n",
    "        t_bal = targets[targets[\"client_id\"].isin(bal_ids)]\n",
    "        print(\"Balanced subset:\")\n",
    "        for t in tcols:\n",
    "            vc = t_bal[t].value_counts().to_dict()\n",
    "            print(f\"  {t}: {vc}\")\n",
    "else:\n",
    "    print(\"(skip) No targets found or no target_* columns.\")\n",
    "\n",
    "# ====== 8) Text length stats (balanced JSONLs) ======\n",
    "print(\"\\n=== Text length stats (balanced JSONLs) ===\")\n",
    "print(\"TRX:\", _text_length_stats(JSON_TRX_BAL))\n",
    "print(\"GEO:\", _text_length_stats(JSON_GEO_BAL))\n",
    "print(\"MM :\", _text_length_stats(JSON_MM_BAL))\n",
    "\n",
    "# ====== 9) Peek a couple of balanced records (trimmed) ======\n",
    "print(\"\\n=== Sample balanced records (trimmed) ===\")\n",
    "print(\"TRX samples:\", _sample_json(JSON_TRX_BAL, k=2))\n",
    "print(\"GEO samples:\", _sample_json(JSON_GEO_BAL, k=2))\n",
    "print(\"MM  samples:\", _sample_json(JSON_MM_BAL, k=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57f90e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MM balanced ids: 2127\n",
      "Labels (no fold yet): 2127\n",
      "Missing fold after merge: 0\n",
      "Final label rows: 2127\n",
      "Fold counts: {0: 425, 1: 423, 2: 419, 3: 446, 4: 414}\n",
      "Targets: ['target_1', 'target_2', 'target_3', 'target_4']\n",
      "Wrote: /Users/tree/Projects/recommemdation_bank/outputs/balanced/labels_mm_folded.parquet\n"
     ]
    }
   ],
   "source": [
    "# Build labels aligned to json_balanced_mm.jsonl and attach real folds from TRX/GEO\n",
    "\n",
    "import re, glob, json\n",
    "import pandas as pd\n",
    "\n",
    "# ====== CONFIG (match 01–04) ======\n",
    "BASE_OUT = \"/Users/tree/Projects/recommemdation_bank/outputs\"\n",
    "JSON_MM_BAL   = f\"{BASE_OUT}/json/mm/json_balanced_mm.jsonl\"\n",
    "TARGETS_RAW   = f\"{BASE_OUT}/balanced/targets_raw.parquet\"\n",
    "OUT_LABELS    = f\"{BASE_OUT}/balanced/labels_mm_folded.parquet\"\n",
    "\n",
    "TRX_GLOB = \"/Users/tree/Projects/recommemdation_bank/data/mbd_mini/detail/trx/fold=*/part-*.parquet\"\n",
    "GEO_GLOB = \"/Users/tree/Projects/recommemdation_bank/data/mbd_mini/detail/geo/fold=*/part-*.parquet\"\n",
    "FOLDS    = [0,1,2,3,4]\n",
    "\n",
    "def load_mm_ids(path):\n",
    "    ids=set()\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            rec = json.loads(line)\n",
    "            cid = str(rec.get(\"client_id\",\"\"))\n",
    "            if cid:\n",
    "                ids.add(cid)\n",
    "    return ids\n",
    "\n",
    "def read_fold_map(glob_pat):\n",
    "    files = sorted(glob.glob(glob_pat))\n",
    "    rows=[]\n",
    "    for p in files:\n",
    "        # infer fold from path\n",
    "        m = re.search(r\"fold=(\\d+)\", p)\n",
    "        fold = int(m.group(1)) if m else -1\n",
    "        # read only client_id\n",
    "        try:\n",
    "            df = pd.read_parquet(p, columns=[\"client_id\"])\n",
    "        except Exception:\n",
    "            df = pd.read_parquet(p)\n",
    "            df = df[[\"client_id\"]]\n",
    "        df[\"client_id\"] = df[\"client_id\"].astype(str)\n",
    "        df[\"fold\"] = fold\n",
    "        rows.append(df)\n",
    "    if not rows:\n",
    "        return pd.DataFrame(columns=[\"client_id\",\"fold\"])\n",
    "    df_all = pd.concat(rows, ignore_index=True)\n",
    "    # choose most common fold per client\n",
    "    return (df_all.groupby([\"client_id\",\"fold\"]).size()\n",
    "                  .reset_index(name=\"n\")\n",
    "                  .sort_values([\"client_id\",\"n\"], ascending=[True,False])\n",
    "                  .drop_duplicates(\"client_id\")[[\"client_id\",\"fold\"]])\n",
    "\n",
    "# 1) ids that actually have MM text\n",
    "mm_ids = load_mm_ids(JSON_MM_BAL)\n",
    "print(\"MM balanced ids:\", len(mm_ids))\n",
    "\n",
    "# 2) base targets (drop to per-client unique)\n",
    "t = pd.read_parquet(TARGETS_RAW)\n",
    "t[\"client_id\"] = t[\"client_id\"].astype(str)\n",
    "target_cols = [c for c in t.columns if c.startswith(\"target_\")]\n",
    "t = t[[\"client_id\"] + target_cols].drop_duplicates(\"client_id\")\n",
    "\n",
    "# keep only those in MM\n",
    "labels = t[t[\"client_id\"].isin(mm_ids)].copy()\n",
    "print(\"Labels (no fold yet):\", len(labels))\n",
    "\n",
    "# 3) fold map from TRX + GEO (majority vote)\n",
    "trx_map = read_fold_map(TRX_GLOB)\n",
    "geo_map = read_fold_map(GEO_GLOB)\n",
    "fold_map = pd.concat([trx_map, geo_map], ignore_index=True)\n",
    "if not fold_map.empty:\n",
    "    fold_map = (fold_map.groupby([\"client_id\",\"fold\"]).size()\n",
    "                        .reset_index(name=\"n\")\n",
    "                        .sort_values([\"client_id\",\"n\"], ascending=[True,False])\n",
    "                        .drop_duplicates(\"client_id\")[[\"client_id\",\"fold\"]])\n",
    "\n",
    "# 4) attach fold; filter to valid folds\n",
    "labels = labels.merge(fold_map, on=\"client_id\", how=\"left\")\n",
    "missing = labels[\"fold\"].isna().sum()\n",
    "print(\"Missing fold after merge:\", int(missing))\n",
    "\n",
    "labels = labels[labels[\"fold\"].isin(FOLDS)].copy()\n",
    "print(\"Final label rows:\", len(labels))\n",
    "print(\"Fold counts:\", labels[\"fold\"].value_counts().sort_index().to_dict())\n",
    "print(\"Targets:\", target_cols)\n",
    "\n",
    "# 5) save\n",
    "labels.to_parquet(OUT_LABELS, index=False)\n",
    "print(\"Wrote:\", OUT_LABELS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
