{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8434cbe8",
   "metadata": {},
   "source": [
    "\n",
    "# Recommendation Dataset Prep — Config & Pipelines\n",
    "\n",
    "These notebooks mirror the MBD-mini preparation flow but are **path- and schema-flexible** for your project.  \n",
    "They will:\n",
    "- Create (or derive) a **balanced client list** (`mbd_targets_balanced.parquet`).\n",
    "- Build per-client **TRX**, **GEO**, and **TRX+GEO** text JSONL datasets.\n",
    "- Filter those JSONLs by the balanced IDs to produce `json_balanced*.jsonl`.\n",
    "- Run quick **sanity checks**.\n",
    "\n",
    "> Default settings match the repo's spirit: TRX cap=256, GEO cap=64 (with consecutive-duplicate collapse), `log10(amount)` formatting, and downsampling to equalize pos/neg per target and fold.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab909de",
   "metadata": {},
   "source": [
    "\n",
    "# 00 — Config & Balanced ID Builder\n",
    "\n",
    "- Set your **paths** below (globs are OK).\n",
    "- Choose how to **get targets**: from a targets file or derive from a **proxy column** in TRX (e.g., `event_type` or `event_subtype`) by picking **top-K categories**.\n",
    "- Builds `mbd_targets_balanced.parquet` under `BASE_OUT/balanced/`.\n",
    "\n",
    "**Your provided examples** (already used as defaults below):\n",
    "- TRX example: `/Users/tree/Projects/recommemdation_bank/data/mbd_mini/detail/trx/fold=0/part-*.parquet`\n",
    "- GEO example: `/Users/tree/Projects/recommemdation_bank/data/mbd_mini/detail/geo/fold=0/part-*.parquet`\n",
    "- You mentioned target path but pointed it to a GEO file by mistake. Below we support both:\n",
    "  1) **TARGETS_FROM_FILE**: if you have a proper targets parquet; or\n",
    "  2) **TARGETS_FROM_PROXY**: derive multilabel targets from TRX `event_type` (or `event_subtype`) by top-K categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d20a218b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRX_GLOB: /Users/tree/Projects/recommemdation_bank/data/mbd_mini/detail/trx/fold=*/part-*.parquet\n",
      "GEO_GLOB: /Users/tree/Projects/recommemdation_bank/data/mbd_mini/detail/geo/fold=*/part-*.parquet\n",
      "TARGETS_PATH: /Users/tree/Projects/recommemdation_bank/data/mbd_mini/targets/fold=*/part-*.parquet\n",
      "BASE_OUT: /Users/tree/Projects/recommemdation_bank/outputs\n",
      "BALANCED_DIR: /Users/tree/Projects/recommemdation_bank/outputs/balanced\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ====== CONFIG ======\n",
    "from pathlib import Path\n",
    "import glob, os\n",
    "\n",
    "# 1) File patterns (globs). Adjust only the \"fold=*\" root if needed.\n",
    "TRX_GLOB = \"/Users/tree/Projects/recommemdation_bank/data/mbd_mini/detail/trx/fold=*/part-*.parquet\"\n",
    "GEO_GLOB = \"/Users/tree/Projects/recommemdation_bank/data/mbd_mini/detail/geo/fold=*/part-*.parquet\"\n",
    "\n",
    "# If you have a targets parquet file/table:\n",
    "TARGETS_PATH = \"/Users/tree/Projects/recommemdation_bank/data/mbd_mini/targets/fold=*/part-*.parquet\"   # e.g., \"/Users/tree/.../targets/part-*.parquet\"  (set to None to derive from proxy)\n",
    "\n",
    "# 2) Folds present\n",
    "FOLDS = [0,1,2,3,4]     # you said you have 4 folds; adjust if different\n",
    "\n",
    "# 3) Output base\n",
    "BASE_OUT = \"/Users/tree/Projects/recommemdation_bank/outputs\"  # change if you prefer\n",
    "os.makedirs(BASE_OUT, exist_ok=True)\n",
    "\n",
    "# 4) Targets derivation policy\n",
    "TARGETS_FROM_FILE  =  True\n",
    "TARGETS_FROM_PROXY = TARGETS_PATH is None  # derive from TRX\n",
    "\n",
    "# If deriving from proxy:\n",
    "PROXY_COLUMN_CANDIDATES = [\"event_subtype\", \"event_type\"]  # will use first one available in TRX\n",
    "TOP_K = 4                     # number of categories -> multilabel targets (target_1..target_K)\n",
    "MIN_CLIENTS_PER_CLASS = 200   # ignore very rare classes\n",
    "\n",
    "# 5) Balancing policy\n",
    "SEED = 42\n",
    "POS_NEG_RATIO = 1.0  # 1.0 means pos:neg = 1:1 downsample; >1 keeps more negatives\n",
    "\n",
    "BALANCED_DIR = f\"{BASE_OUT}/balanced\"\n",
    "os.makedirs(BALANCED_DIR, exist_ok=True)\n",
    "\n",
    "print(\"TRX_GLOB:\", TRX_GLOB)\n",
    "print(\"GEO_GLOB:\", GEO_GLOB)\n",
    "print(\"TARGETS_PATH:\", TARGETS_PATH)\n",
    "print(\"BASE_OUT:\", BASE_OUT)\n",
    "print(\"BALANCED_DIR:\", BALANCED_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1399074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets from file: (100224, 6) with 4 target columns\n",
      "Saved raw targets to: /Users/tree/Projects/recommemdation_bank/outputs/balanced/targets_raw.parquet\n",
      "Balanced IDs saved to: /Users/tree/Projects/recommemdation_bank/outputs/balanced/mbd_targets_balanced.parquet\n",
      "Num balanced clients: 2132\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ====== BUILD TARGETS (from file OR derive from TRX proxy) ======\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "def read_parquets(pattern, columns=None, limit_files=None):\n",
    "    paths = sorted(glob.glob(pattern))\n",
    "    if limit_files is not None:\n",
    "        paths = paths[:limit_files]\n",
    "    if not paths:\n",
    "        raise FileNotFoundError(f\"No files matched: {pattern}\")\n",
    "    dfs = []\n",
    "    for p in paths:\n",
    "        try:\n",
    "            df = pd.read_parquet(p, columns=columns)\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"SKIP {p}: {e}\")\n",
    "    if not dfs:\n",
    "        raise RuntimeError(f\"No readable parquet files for {pattern}\")\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# 1) Targets from file\n",
    "if TARGETS_FROM_FILE:\n",
    "    tdf = read_parquets(TARGETS_PATH)\n",
    "    # Expect client_id, fold, and columns like target_*\n",
    "    target_cols = [c for c in tdf.columns if c.startswith(\"target_\")]\n",
    "    if not target_cols:\n",
    "        raise ValueError(\"Targets file has no columns starting with 'target_'\")\n",
    "    assert \"client_id\" in tdf.columns, \"Targets must include 'client_id'\"\n",
    "    if \"fold\" not in tdf.columns:\n",
    "        # best-effort: try to infer or set a dummy fold\n",
    "        tdf[\"fold\"] = -1\n",
    "    targets = tdf[[\"client_id\",\"fold\"] + target_cols].drop_duplicates(\"client_id\")\n",
    "    print(\"Targets from file:\", targets.shape, \"with\", len(target_cols), \"target columns\")\n",
    "else:\n",
    "    # 2) Derive targets from TRX proxy\n",
    "    #    Strategy: pick PROXY_COLUMN from candidates; find top-K frequent values;\n",
    "    #    build multilabel targets: target_i = 1 if client has that category anywhere in TRX.\n",
    "    trx_cols_try = [\"client_id\", \"fold\"] + list(set(PROXY_COLUMN_CANDIDATES))\n",
    "    trx = read_parquets(TRX_GLOB, columns=None)  # read all columns; will subset later\n",
    "    existing = [c for c in trx_cols_try if c in trx.columns]\n",
    "    missing = [c for c in trx_cols_try if c not in trx.columns]\n",
    "    print(\"Existing columns:\", existing, \"| Missing:\", missing)\n",
    "    if not any(c in trx.columns for c in PROXY_COLUMN_CANDIDATES):\n",
    "        raise ValueError(f\"None of proxy columns found: {PROXY_COLUMN_CANDIDATES} in TRX\")\n",
    "    # choose the first available proxy column\n",
    "    proxy_col = next(c for c in PROXY_COLUMN_CANDIDATES if c in trx.columns)\n",
    "    print(\"Using proxy column:\", proxy_col)\n",
    "\n",
    "    # Keep only needed columns\n",
    "    base_cols = [\"client_id\", \"fold\", proxy_col]\n",
    "    trx = trx[base_cols].dropna(subset=[\"client_id\", proxy_col])\n",
    "    # For safety, fold fill\n",
    "    if \"fold\" not in trx.columns:\n",
    "        trx[\"fold\"] = -1\n",
    "\n",
    "    # Top-K classes with enough clients\n",
    "    # Count unique clients per category\n",
    "    cat_clients = trx.groupby(proxy_col)[\"client_id\"].nunique().sort_values(ascending=False)\n",
    "    cat_kept = cat_clients[cat_clients >= MIN_CLIENTS_PER_CLASS].head(TOP_K).index.tolist()\n",
    "    if not cat_kept:\n",
    "        raise ValueError(\"No proxy categories have enough clients. Lower MIN_CLIENTS_PER_CLASS or check data.\")\n",
    "    print(\"Top categories:\", cat_kept)\n",
    "\n",
    "    # Build multilabel targets per client\n",
    "    labels = []\n",
    "    for cat in cat_kept:\n",
    "        d = trx.loc[trx[proxy_col] == cat, [\"client_id\"]].drop_duplicates()\n",
    "        d[cat] = 1\n",
    "        labels.append(d.rename(columns={cat: f\"target_{len(labels)+1}\"}))\n",
    "    lab = labels[0]\n",
    "    for i in range(1, len(labels)):\n",
    "        lab = lab.merge(labels[i], on=\"client_id\", how=\"outer\")\n",
    "    lab = lab.fillna(0)\n",
    "\n",
    "    # Fold assignment per client: choose the most frequent fold seen in TRX\n",
    "    fold_map = (trx.groupby([\"client_id\",\"fold\"]).size()\n",
    "                  .reset_index(name=\"n\")\n",
    "                  .sort_values([\"client_id\",\"n\"], ascending=[True, False])\n",
    "                  .drop_duplicates(\"client_id\")[[\"client_id\",\"fold\"]])\n",
    "    targets = lab.merge(fold_map, on=\"client_id\", how=\"left\")\n",
    "    # reorder columns\n",
    "    target_cols = [c for c in targets.columns if c.startswith(\"target_\")]\n",
    "    targets = targets[[\"client_id\",\"fold\"] + target_cols]\n",
    "    print(\"Derived targets shape:\", targets.shape, \"| targets:\", target_cols)\n",
    "\n",
    "# Save raw targets for reference\n",
    "raw_targets_path = f\"{BALANCED_DIR}/targets_raw.parquet\"\n",
    "targets.to_parquet(raw_targets_path, index=False)\n",
    "print(\"Saved raw targets to:\", raw_targets_path)\n",
    "\n",
    "# ====== Compute balanced client_ids ======\n",
    "keep_ids = set()\n",
    "target_cols = [c for c in targets.columns if c.startswith(\"target_\")]\n",
    "if not target_cols:\n",
    "    raise ValueError(\"No target_* columns found to balance on.\")\n",
    "\n",
    "for fold in sorted(targets[\"fold\"].dropna().unique()):\n",
    "    d = targets[targets[\"fold\"] == fold]\n",
    "    for t in target_cols:\n",
    "        pos = d.loc[d[t] == 1, \"client_id\"].dropna().unique()\n",
    "        neg = d.loc[d[t] == 0, \"client_id\"].dropna().unique()\n",
    "        if len(pos) == 0 or len(neg) == 0:\n",
    "            print(f\"[WARN] fold={fold}, {t}: pos={len(pos)}, neg={len(neg)} -> skip\")\n",
    "            continue\n",
    "        k = min(len(pos), int(len(neg) / POS_NEG_RATIO))\n",
    "        if k == 0:\n",
    "            print(f\"[WARN] fold={fold}, {t}: computed k=0 -> skip\")\n",
    "            continue\n",
    "        keep_ids.update(rng.choice(pos, k, replace=False))\n",
    "        keep_ids.update(rng.choice(neg, k, replace=False))\n",
    "\n",
    "balanced_ids = pd.DataFrame({\"client_id\": sorted(keep_ids)})\n",
    "balanced_path = f\"{BALANCED_DIR}/mbd_targets_balanced.parquet\"\n",
    "balanced_ids.to_parquet(balanced_path, index=False)\n",
    "\n",
    "print(f\"Balanced IDs saved to: {balanced_path}\")\n",
    "print(\"Num balanced clients:\", len(balanced_ids))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
