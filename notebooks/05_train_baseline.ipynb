{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94a95748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data rows: 2127 | Targets: ['target_1', 'target_2', 'target_3', 'target_4']\n",
      "Fold counts: {0: 425, 1: 423, 2: 419, 3: 446, 4: 414}\n",
      "\n",
      "=== Fold 0 | train=1702 valid=425 ===\n",
      "\n",
      "=== Fold 1 | train=1704 valid=423 ===\n",
      "\n",
      "=== Fold 2 | train=1708 valid=419 ===\n",
      "\n",
      "=== Fold 3 | train=1681 valid=446 ===\n",
      "\n",
      "=== Fold 4 | train=1713 valid=414 ===\n",
      "\n",
      "=== Per-target metrics (mean±std across folds) ===\n",
      "  target  AUC_mean  AP_mean  LL_mean  pos_valid  n_valid\n",
      "target_1    0.6596   0.4009   0.6350        556     2127\n",
      "target_2    0.7477   0.3377   0.2790         33     2127\n",
      "target_3    0.6448   0.2142   0.6115        276     2127\n",
      "target_4    0.7389   0.3380   0.5253        237     2127\n",
      "   MACRO    0.6977   0.3227   0.5127       1102     8508\n",
      "\n",
      "=== Ranking metrics (averaged across folds) ===\n",
      " users_eval  Hit@1  Hit@3  NDCG@3  NDCG@5\n",
      "     1039.0 0.4727 0.9526  0.7516  0.7742\n",
      "\n",
      "Saved metrics to: /Users/tree/Projects/recommemdation_bank/outputs/metrics\n"
     ]
    }
   ],
   "source": [
    "# 05_train_baseline.py — TF-IDF + LogisticRegression baseline with 5-fold CV\n",
    "\n",
    "import os, json, math, re, glob, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, log_loss\n",
    "\n",
    "# ===================== CONFIG =====================\n",
    "BASE_OUT = \"/Users/tree/Projects/recommemdation_bank/outputs\"\n",
    "\n",
    "MM_JSONL   = f\"{BASE_OUT}/json/mm/json_balanced_mm.jsonl\"\n",
    "LABELS_PAR = f\"{BASE_OUT}/balanced/labels_mm_folded.parquet\"   # created in 04\n",
    "\n",
    "RESULTS_DIR = f\"{BASE_OUT}/metrics\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "FOLDS = [0,1,2,3,4]\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# TF-IDF settings — tuned for your tokenized text format (<TRX>/<GEO> rows)\n",
    "VEC_KW = dict(\n",
    "    max_features=200_000,\n",
    "    ngram_range=(1,2),          # unigrams + bigrams\n",
    "    min_df=2,                   # ignore extremely rare tokens\n",
    "    max_df=0.995,\n",
    "    lowercase=True,\n",
    "    preprocessor=lambda s: s.replace('.', '_')  # keep a6.24 etc. as one token (a6_24)\n",
    ")\n",
    "\n",
    "LR_KW = dict(\n",
    "    solver=\"liblinear\",         # robust on small datasets & sparse mats\n",
    "    C=1.0,\n",
    "    class_weight=\"balanced\",\n",
    "    max_iter=300,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# ===================== HELPERS =====================\n",
    "def load_mm_texts(jsonl_path):\n",
    "    rows=[]\n",
    "    with open(jsonl_path) as f:\n",
    "        for line in f:\n",
    "            r = json.loads(line)\n",
    "            rows.append((str(r[\"client_id\"]), r[\"text\"]))\n",
    "    return pd.DataFrame(rows, columns=[\"client_id\",\"text\"])\n",
    "\n",
    "def ndcg_at_k(y_true_row, y_score_row, k):\n",
    "    \"\"\"y_true_row: 1D {0,1} array; y_score_row: 1D scores; both length = #targets.\"\"\"\n",
    "    k = min(k, len(y_true_row))\n",
    "    if k <= 0: return np.nan\n",
    "    # order by predicted score\n",
    "    order = np.argsort(-y_score_row)\n",
    "    rel_k = np.take(y_true_row, order[:k])\n",
    "    # DCG\n",
    "    discounts = 1.0 / np.log2(np.arange(2, k+2))\n",
    "    dcg = np.sum((2**rel_k - 1) * discounts)\n",
    "    # IDCG\n",
    "    ideal = np.sort(y_true_row)[::-1][:k]\n",
    "    idcg = np.sum((2**ideal - 1) * discounts)\n",
    "    if idcg == 0:\n",
    "        return np.nan\n",
    "    return dcg / idcg\n",
    "\n",
    "def hit_at_k(y_true_row, y_score_row, k):\n",
    "    \"\"\"1 if any positive label is in top-k predictions, else 0; nan if no positives.\"\"\"\n",
    "    if y_true_row.sum() == 0:\n",
    "        return np.nan\n",
    "    order = np.argsort(-y_score_row)[:k]\n",
    "    return 1.0 if y_true_row[order].sum() > 0 else 0.0\n",
    "\n",
    "def safe_auc(y_true, y_prob):\n",
    "    y = np.asarray(y_true)\n",
    "    if len(np.unique(y)) < 2:\n",
    "        return np.nan\n",
    "    return roc_auc_score(y, y_prob)\n",
    "\n",
    "def safe_ap(y_true, y_prob):\n",
    "    y = np.asarray(y_true)\n",
    "    if len(np.unique(y)) < 2:\n",
    "        return np.nan\n",
    "    return average_precision_score(y, y_prob)\n",
    "\n",
    "def safe_logloss(y_true, y_prob):\n",
    "    y = np.asarray(y_true)\n",
    "    # clip to avoid -inf\n",
    "    p = np.clip(y_prob, 1e-6, 1-1e-6)\n",
    "    if len(np.unique(y)) < 2:\n",
    "        return np.nan\n",
    "    try:\n",
    "        return log_loss(y, p, labels=[0,1])\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "# ===================== LOAD DATA =====================\n",
    "texts  = load_mm_texts(MM_JSONL)\n",
    "labels = pd.read_parquet(LABELS_PAR)\n",
    "labels[\"client_id\"] = labels[\"client_id\"].astype(str)\n",
    "\n",
    "df = labels.merge(texts, on=\"client_id\", how=\"inner\")\n",
    "target_cols = [c for c in df.columns if c.startswith(\"target_\")]\n",
    "assert \"fold\" in df.columns, \"labels_mm_folded.parquet must have a 'fold' column.\"\n",
    "print(\"Data rows:\", len(df), \"| Targets:\", target_cols)\n",
    "print(\"Fold counts:\", df[\"fold\"].value_counts().sort_index().to_dict())\n",
    "\n",
    "# ===================== CV TRAIN / EVAL =====================\n",
    "all_fold_rows = []         # per-target metrics\n",
    "all_rank_rows = []         # per-user ranking metrics aggregated per fold\n",
    "\n",
    "for fold in FOLDS:\n",
    "    train = df[df.fold != fold].reset_index(drop=True)\n",
    "    valid = df[df.fold == fold].reset_index(drop=True)\n",
    "\n",
    "    print(f\"\\n=== Fold {fold} | train={len(train)} valid={len(valid)} ===\")\n",
    "\n",
    "    # Vectorize on training text only\n",
    "    vec = TfidfVectorizer(**VEC_KW)\n",
    "    X_tr = vec.fit_transform(train[\"text\"])\n",
    "    X_va = vec.transform(valid[\"text\"])\n",
    "\n",
    "    # Collect predictions for ranking metrics\n",
    "    Y_va = valid[target_cols].values.astype(int)\n",
    "    S_va = np.zeros_like(Y_va, dtype=float)\n",
    "\n",
    "    # Per-target binary classifier\n",
    "    for ti, t in enumerate(target_cols):\n",
    "        y_tr = train[t].values.astype(int)\n",
    "        y_va = valid[t].values.astype(int)\n",
    "\n",
    "        clf = LogisticRegression(**LR_KW)\n",
    "        clf.fit(X_tr, y_tr)\n",
    "        pr_va = clf.predict_proba(X_va)[:, 1]\n",
    "        S_va[:, ti] = pr_va\n",
    "\n",
    "        # Metrics for this target\n",
    "        auc  = safe_auc(y_va, pr_va)\n",
    "        ap   = safe_ap(y_va, pr_va)\n",
    "        ll   = safe_logloss(y_va, pr_va)\n",
    "\n",
    "        all_fold_rows.append({\n",
    "            \"fold\": fold, \"target\": t,\n",
    "            \"AUC\": auc, \"AP\": ap, \"LogLoss\": ll,\n",
    "            \"pos_valid\": int(y_va.sum()), \"n_valid\": int(len(y_va))\n",
    "        })\n",
    "\n",
    "    # Ranking-style metrics across targets per user (only evaluate users with ≥1 positive)\n",
    "    hits1, hits3, ndcg3, ndcg5 = [], [], [], []\n",
    "    for i in range(len(valid)):\n",
    "        y_row = Y_va[i]\n",
    "        s_row = S_va[i]\n",
    "        if y_row.sum() == 0:\n",
    "            continue\n",
    "        hits1.append(hit_at_k(y_row, s_row, 1))\n",
    "        hits3.append(hit_at_k(y_row, s_row, 3))\n",
    "        ndcg3.append(ndcg_at_k(y_row, s_row, 3))\n",
    "        ndcg5.append(ndcg_at_k(y_row, s_row, 5))\n",
    "\n",
    "    rank_row = {\n",
    "        \"fold\": fold,\n",
    "        \"users_eval\": int(len(hits1)),   # users with at least one positive label\n",
    "        \"Hit@1\": float(np.nanmean(hits1)) if hits1 else np.nan,\n",
    "        \"Hit@3\": float(np.nanmean(hits3)) if hits3 else np.nan,\n",
    "        \"NDCG@3\": float(np.nanmean(ndcg3)) if ndcg3 else np.nan,\n",
    "        \"NDCG@5\": float(np.nanmean(ndcg5)) if ndcg5 else np.nan,\n",
    "    }\n",
    "    all_rank_rows.append(rank_row)\n",
    "\n",
    "    # free memory\n",
    "    del X_tr, X_va, vec\n",
    "    gc.collect()\n",
    "\n",
    "# ===================== SUMMARIZE =====================\n",
    "metrics_df = pd.DataFrame(all_fold_rows)\n",
    "rank_df    = pd.DataFrame(all_rank_rows)\n",
    "\n",
    "# per-target averages across folds\n",
    "summary_target = (metrics_df\n",
    "                  .groupby(\"target\", as_index=False)\n",
    "                  .agg(AUC_mean=(\"AUC\", \"mean\"),\n",
    "                       AUC_std =(\"AUC\", \"std\"),\n",
    "                       AP_mean =(\"AP\", \"mean\"),\n",
    "                       AP_std  =(\"AP\", \"std\"),\n",
    "                       LL_mean =(\"LogLoss\", \"mean\"),\n",
    "                       LL_std  =(\"LogLoss\", \"std\"),\n",
    "                       pos_valid=(\"pos_valid\",\"sum\"),\n",
    "                       n_valid  =(\"n_valid\",\"sum\"))\n",
    "                 )\n",
    "\n",
    "# macro averages across targets\n",
    "macro_row = {\n",
    "    \"target\": \"MACRO\",\n",
    "    \"AUC_mean\": summary_target[\"AUC_mean\"].mean(),\n",
    "    \"AUC_std\":  summary_target[\"AUC_mean\"].std(),\n",
    "    \"AP_mean\":  summary_target[\"AP_mean\"].mean(),\n",
    "    \"AP_std\":   summary_target[\"AP_mean\"].std(),\n",
    "    \"LL_mean\":  summary_target[\"LL_mean\"].mean(),\n",
    "    \"LL_std\":   summary_target[\"LL_mean\"].std(),\n",
    "    \"pos_valid\": int(summary_target[\"pos_valid\"].sum()),\n",
    "    \"n_valid\":   int(summary_target[\"n_valid\"].sum()),\n",
    "}\n",
    "summary_target = pd.concat([summary_target, pd.DataFrame([macro_row])], ignore_index=True)\n",
    "\n",
    "# ranking averages\n",
    "summary_rank = (rank_df\n",
    "                .agg({\"users_eval\":\"sum\",\n",
    "                      \"Hit@1\":\"mean\",\"Hit@3\":\"mean\",\n",
    "                      \"NDCG@3\":\"mean\",\"NDCG@5\":\"mean\"})\n",
    "                .to_frame(name=\"mean\").T)\n",
    "\n",
    "print(\"\\n=== Per-target metrics (mean±std across folds) ===\")\n",
    "print(summary_target[[\"target\",\"AUC_mean\",\"AP_mean\",\"LL_mean\",\"pos_valid\",\"n_valid\"]]\n",
    "      .round(4).to_string(index=False))\n",
    "\n",
    "print(\"\\n=== Ranking metrics (averaged across folds) ===\")\n",
    "print(summary_rank.round(4).to_string(index=False))\n",
    "\n",
    "# ===================== SAVE =====================\n",
    "metrics_df.to_csv(f\"{RESULTS_DIR}/fold_metrics_per_target.csv\", index=False)\n",
    "rank_df.to_csv(f\"{RESULTS_DIR}/fold_metrics_ranking.csv\", index=False)\n",
    "summary_target.to_csv(f\"{RESULTS_DIR}/summary_per_target.csv\", index=False)\n",
    "summary_rank.to_csv(f\"{RESULTS_DIR}/summary_ranking.csv\", index=False)\n",
    "\n",
    "print(\"\\nSaved metrics to:\", RESULTS_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
