{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0b342f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data rows: 2127 | Targets: ['target_1', 'target_2', 'target_3', 'target_4']\n",
      "Fold counts: {0: 425, 1: 423, 2: 419, 3: 446, 4: 414}\n",
      "\n",
      "=== Fold 0 | train=1702 valid=425 ===\n",
      "\n",
      "=== Fold 1 | train=1704 valid=423 ===\n",
      "\n",
      "=== Fold 2 | train=1708 valid=419 ===\n",
      "\n",
      "=== Fold 3 | train=1681 valid=446 ===\n",
      "\n",
      "=== Fold 4 | train=1713 valid=414 ===\n",
      "\n",
      "=== Macro (avg over targets) ===\n",
      "                 model  AUC_macro  AP_macro  LL_macro\n",
      "   Logistic Regression     0.7266    0.3379    0.6711\n",
      "                   LDA     0.7061    0.3063    0.4321\n",
      "         Random Forest     0.7027    0.3072    0.3399\n",
      "         MultinomialNB     0.6950    0.3153    0.9757\n",
      "     Gradient Boosting     0.6948    0.2917    0.4058\n",
      "Support Vector Machine     0.6899    0.3146    0.3269\n",
      "  MLP (Neural Network)     0.6789    0.2951    0.5008\n",
      "              AdaBoost     0.6490    0.2375    0.6198\n",
      "   K-Nearest Neighbors     0.5974    0.2022    0.8061\n",
      "         Decision Tree     0.5625    0.1577    3.2798\n",
      "\n",
      "=== Ranking (avg over folds) ===\n",
      "                 model  users_eval   Hit1   Hit3  NDCG3  NDCG5\n",
      "Support Vector Machine        1039 0.5636 0.9800 0.8072 0.8166\n",
      "                   LDA        1039 0.5487 0.9789 0.8004 0.8100\n",
      "         Random Forest        1039 0.5524 0.9770 0.7973 0.8082\n",
      "     Gradient Boosting        1039 0.5059 0.9762 0.7773 0.7886\n",
      "              AdaBoost        1039 0.4564 0.9697 0.7503 0.7644\n",
      "   Logistic Regression        1039 0.4678 0.9686 0.7590 0.7742\n",
      "         MultinomialNB        1039 0.4679 0.9513 0.7476 0.7710\n",
      "   K-Nearest Neighbors        1039 0.4956 0.8790 0.7102 0.7697\n",
      "  MLP (Neural Network)        1039 0.4839 0.8761 0.7056 0.7642\n",
      "         Decision Tree        1039 0.5055 0.8456 0.6900 0.7619\n",
      "\n",
      "Saved results to: /Users/tree/Projects/recommemdation_bank/outputs/metrics/model_zoo\n"
     ]
    }
   ],
   "source": [
    "# 05_train_model_zoo.py — Compare your classifier list (with MultinomialNB) on 5-fold CV\n",
    "\n",
    "import os, json, gc, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, log_loss\n",
    "from sklearn.base import clone\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ===================== PATHS & CONFIG =====================\n",
    "BASE_OUT   = \"/Users/tree/Projects/recommemdation_bank/outputs\"\n",
    "MM_JSONL   = f\"{BASE_OUT}/json/mm/json_balanced_mm.jsonl\"\n",
    "LABELS_PAR = f\"{BASE_OUT}/balanced/labels_mm_folded.parquet\"\n",
    "\n",
    "RESULTS_DIR = f\"{BASE_OUT}/metrics/model_zoo\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "FOLDS = [0,1,2,3,4]\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Sparse TF-IDF\n",
    "WORD_VECT = dict(\n",
    "    max_features=200_000,\n",
    "    ngram_range=(1,2),\n",
    "    min_df=2, max_df=0.995,\n",
    "    lowercase=True,\n",
    "    sublinear_tf=True,\n",
    "    preprocessor=lambda s: s.replace('.', '_')  # keep a6.24 → a6_24\n",
    ")\n",
    "CHAR_VECT = dict(\n",
    "    analyzer=\"char_wb\",\n",
    "    ngram_range=(3,5),\n",
    "    min_df=2, max_df=1.0,\n",
    "    lowercase=True\n",
    ")\n",
    "\n",
    "SVD_DIMS = 512  # dense projection size for dense-only models\n",
    "\n",
    "# ===================== HELPERS =====================\n",
    "def load_mm_texts(path):\n",
    "    rows=[]\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            r = json.loads(line)\n",
    "            rows.append((str(r[\"client_id\"]), r[\"text\"]))\n",
    "    return pd.DataFrame(rows, columns=[\"client_id\",\"text\"])\n",
    "\n",
    "def ndcg_at_k(y_true_row, y_score_row, k):\n",
    "    k = min(k, len(y_true_row))\n",
    "    if k <= 0: return np.nan\n",
    "    order = np.argsort(-y_score_row)\n",
    "    rel_k = np.take(y_true_row, order[:k])\n",
    "    discounts = 1.0 / np.log2(np.arange(2, k+2))\n",
    "    dcg  = np.sum((2**rel_k - 1) * discounts)\n",
    "    ideal = np.sort(y_true_row)[::-1][:k]\n",
    "    idcg = np.sum((2**ideal - 1) * discounts)\n",
    "    return np.nan if idcg == 0 else dcg / idcg\n",
    "\n",
    "def hit_at_k(y_true_row, y_score_row, k):\n",
    "    if y_true_row.sum() == 0: return np.nan\n",
    "    order = np.argsort(-y_score_row)[:k]\n",
    "    return 1.0 if y_true_row[order].sum() > 0 else 0.0\n",
    "\n",
    "def safe_auc(y_true, y_prob):\n",
    "    y = np.asarray(y_true)\n",
    "    return np.nan if len(np.unique(y)) < 2 else roc_auc_score(y, y_prob)\n",
    "\n",
    "def safe_ap(y_true, y_prob):\n",
    "    y = np.asarray(y_true)\n",
    "    return np.nan if len(np.unique(y)) < 2 else average_precision_score(y, y_prob)\n",
    "\n",
    "def safe_logloss(y_true, y_prob):\n",
    "    y = np.asarray(y_true)\n",
    "    p = np.clip(y_prob, 1e-6, 1-1e-6)\n",
    "    return np.nan if len(np.unique(y)) < 2 else log_loss(y, p, labels=[0,1])\n",
    "\n",
    "def build_features(train_text, valid_text):\n",
    "    # sparse: word + char TF-IDF\n",
    "    vec_w = TfidfVectorizer(**WORD_VECT)\n",
    "    vec_c = TfidfVectorizer(**CHAR_VECT)\n",
    "    Xtr_w = vec_w.fit_transform(train_text)\n",
    "    Xva_w = vec_w.transform(valid_text)\n",
    "    Xtr_c = vec_c.fit_transform(train_text)\n",
    "    Xva_c = vec_c.transform(valid_text)\n",
    "    X_tr_sparse = hstack([Xtr_w, Xtr_c], format=\"csr\")\n",
    "    X_va_sparse = hstack([Xva_w, Xva_c], format=\"csr\")\n",
    "    # dense: SVD -> Standardize (with_mean=True) for LDA/MLP/SVC/KNN/etc.\n",
    "    svd = TruncatedSVD(n_components=SVD_DIMS, random_state=RANDOM_STATE)\n",
    "    X_tr_dense = svd.fit_transform(X_tr_sparse)\n",
    "    X_va_dense = svd.transform(X_va_sparse)\n",
    "    scaler = StandardScaler(with_mean=True)\n",
    "    X_tr_dense = scaler.fit_transform(X_tr_dense)\n",
    "    X_va_dense = scaler.transform(X_va_dense)\n",
    "    return (X_tr_sparse, X_va_sparse), (X_tr_dense, X_va_dense)\n",
    "\n",
    "def class_weights(y):\n",
    "    pos = (y == 1).sum()\n",
    "    neg = (y == 0).sum()\n",
    "    if pos == 0 or neg == 0:\n",
    "        return None\n",
    "    w_pos = neg / max(pos, 1)\n",
    "    return np.where(y == 1, w_pos, 1.0)\n",
    "\n",
    "# ===================== CLASSIFIERS =====================\n",
    "# Your list, with GaussianNB → MultinomialNB\n",
    "sparse_models = {\n",
    "    \"Logistic Regression\": LogisticRegression(\n",
    "        solver=\"liblinear\", C=1.0, class_weight=\"balanced\", max_iter=300, random_state=RANDOM_STATE\n",
    "    ),\n",
    "    \"MultinomialNB\": MultinomialNB(alpha=1.0)\n",
    "}\n",
    "\n",
    "dense_models = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=RANDOM_STATE),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Support Vector Machine\": SVC(kernel=\"rbf\", probability=True, random_state=RANDOM_STATE),\n",
    "    \"LDA\": LinearDiscriminantAnalysis(),\n",
    "    \"MLP (Neural Network)\": MLPClassifier(max_iter=1000, random_state=RANDOM_STATE)\n",
    "}\n",
    "\n",
    "# ===================== LOAD =====================\n",
    "texts  = load_mm_texts(MM_JSONL)\n",
    "labels = pd.read_parquet(LABELS_PAR)\n",
    "labels[\"client_id\"] = labels[\"client_id\"].astype(str)\n",
    "df = labels.merge(texts, on=\"client_id\", how=\"inner\")\n",
    "\n",
    "target_cols = [c for c in df.columns if c.startswith(\"target_\")]\n",
    "assert \"fold\" in df.columns\n",
    "print(\"Data rows:\", len(df), \"| Targets:\", target_cols)\n",
    "print(\"Fold counts:\", df[\"fold\"].value_counts().sort_index().to_dict())\n",
    "\n",
    "# ===================== TRAIN / EVAL =====================\n",
    "all_metrics_rows = []\n",
    "all_rank_rows    = []\n",
    "\n",
    "for fold in FOLDS:\n",
    "    train = df[df.fold != fold].reset_index(drop=True)\n",
    "    valid = df[df.fold == fold].reset_index(drop=True)\n",
    "    print(f\"\\n=== Fold {fold} | train={len(train)} valid={len(valid)} ===\")\n",
    "\n",
    "    (X_tr_sparse, X_va_sparse), (X_tr_dense, X_va_dense) = build_features(train[\"text\"], valid[\"text\"])\n",
    "    Y_tr = train[target_cols].values.astype(int)\n",
    "    Y_va = valid[target_cols].values.astype(int)\n",
    "\n",
    "    # ---- sparse-friendly ----\n",
    "    for mname, model in sparse_models.items():\n",
    "        S_va = np.zeros_like(Y_va, dtype=float)\n",
    "        for ti, t in enumerate(target_cols):\n",
    "            y_tr = Y_tr[:, ti]\n",
    "            weights = class_weights(y_tr)\n",
    "\n",
    "            clf = clone(model)\n",
    "            # MultinomialNB & LR both handle sparse; pass sample_weight if supported\n",
    "            try:\n",
    "                clf.fit(X_tr_sparse, y_tr, sample_weight=weights)\n",
    "            except TypeError:\n",
    "                clf.fit(X_tr_sparse, y_tr)\n",
    "\n",
    "            if hasattr(clf, \"predict_proba\"):\n",
    "                pr = clf.predict_proba(X_va_sparse)[:, 1]\n",
    "            elif hasattr(clf, \"decision_function\"):\n",
    "                pr = 1.0 / (1.0 + np.exp(-clf.decision_function(X_va_sparse)))\n",
    "            else:\n",
    "                pr = clf.predict(X_va_sparse).astype(float)\n",
    "\n",
    "            S_va[:, ti] = pr\n",
    "            all_metrics_rows.append({\n",
    "                \"model\": mname, \"fold\": fold, \"target\": t,\n",
    "                \"AUC\": safe_auc(Y_va[:, ti], pr),\n",
    "                \"AP\":  safe_ap(Y_va[:, ti], pr),\n",
    "                \"LogLoss\": safe_logloss(Y_va[:, ti], pr),\n",
    "                \"pos_valid\": int(Y_va[:, ti].sum()), \"n_valid\": int(len(Y_va))\n",
    "            })\n",
    "\n",
    "        # ranking\n",
    "        hits1, hits3, ndcg3, ndcg5 = [], [], [], []\n",
    "        for i in range(len(valid)):\n",
    "            y_row, s_row = Y_va[i], S_va[i]\n",
    "            if y_row.sum() == 0: continue\n",
    "            hits1.append(hit_at_k(y_row, s_row, 1))\n",
    "            hits3.append(hit_at_k(y_row, s_row, 3))\n",
    "            ndcg3.append(ndcg_at_k(y_row, s_row, 3))\n",
    "            ndcg5.append(ndcg_at_k(y_row, s_row, 5))\n",
    "        all_rank_rows.append({\n",
    "            \"model\": mname, \"fold\": fold,\n",
    "            \"users_eval\": int(len(hits1)),\n",
    "            \"Hit@1\": float(np.nanmean(hits1)) if hits1 else np.nan,\n",
    "            \"Hit@3\": float(np.nanmean(hits3)) if hits3 else np.nan,\n",
    "            \"NDCG@3\": float(np.nanmean(ndcg3)) if ndcg3 else np.nan,\n",
    "            \"NDCG@5\": float(np.nanmean(ndcg5)) if ndcg5 else np.nan,\n",
    "        })\n",
    "\n",
    "    # ---- dense-only ----\n",
    "    for mname, model in dense_models.items():\n",
    "        S_va = np.zeros_like(Y_va, dtype=float)\n",
    "        for ti, t in enumerate(target_cols):\n",
    "            y_tr = Y_tr[:, ti]\n",
    "            weights = class_weights(y_tr)\n",
    "            clf = clone(model)\n",
    "\n",
    "            try:\n",
    "                clf.fit(X_tr_dense, y_tr, sample_weight=weights)\n",
    "            except TypeError:\n",
    "                clf.fit(X_tr_dense, y_tr)\n",
    "\n",
    "            if hasattr(clf, \"predict_proba\"):\n",
    "                pr = clf.predict_proba(X_va_dense)[:, 1]\n",
    "            elif hasattr(clf, \"decision_function\"):\n",
    "                dfc = clf.decision_function(X_va_dense)\n",
    "                pr  = 1.0 / (1.0 + np.exp(-dfc))\n",
    "            else:\n",
    "                pr = clf.predict(X_va_dense).astype(float)\n",
    "\n",
    "            S_va[:, ti] = pr\n",
    "            all_metrics_rows.append({\n",
    "                \"model\": mname, \"fold\": fold, \"target\": t,\n",
    "                \"AUC\": safe_auc(Y_va[:, ti], pr),\n",
    "                \"AP\":  safe_ap(Y_va[:, ti], pr),\n",
    "                \"LogLoss\": safe_logloss(Y_va[:, ti], pr),\n",
    "                \"pos_valid\": int(Y_va[:, ti].sum()), \"n_valid\": int(len(Y_va))\n",
    "            })\n",
    "\n",
    "        hits1, hits3, ndcg3, ndcg5 = [], [], [], []\n",
    "        for i in range(len(valid)):\n",
    "            y_row, s_row = Y_va[i], S_va[i]\n",
    "            if y_row.sum() == 0: continue\n",
    "            hits1.append(hit_at_k(y_row, s_row, 1))\n",
    "            hits3.append(hit_at_k(y_row, s_row, 3))\n",
    "            ndcg3.append(ndcg_at_k(y_row, s_row, 3))\n",
    "            ndcg5.append(ndcg_at_k(y_row, s_row, 5))\n",
    "        all_rank_rows.append({\n",
    "            \"model\": mname, \"fold\": fold,\n",
    "            \"users_eval\": int(len(hits1)),\n",
    "            \"Hit@1\": float(np.nanmean(hits1)) if hits1 else np.nan,\n",
    "            \"Hit@3\": float(np.nanmean(hits3)) if hits3 else np.nan,\n",
    "            \"NDCG@3\": float(np.nanmean(ndcg3)) if ndcg3 else np.nan,\n",
    "            \"NDCG@5\": float(np.nanmean(ndcg5)) if ndcg5 else np.nan,\n",
    "        })\n",
    "\n",
    "    # free memory\n",
    "    del X_tr_sparse, X_va_sparse, X_tr_dense, X_va_dense, Y_tr, Y_va\n",
    "    gc.collect()\n",
    "\n",
    "# ===================== SUMMARIZE & SAVE =====================\n",
    "metrics_df = pd.DataFrame(all_metrics_rows)\n",
    "rank_df    = pd.DataFrame(all_rank_rows)\n",
    "\n",
    "summary_per_model_target = (metrics_df\n",
    "    .groupby([\"model\",\"target\"], as_index=False)\n",
    "    .agg(AUC_mean=(\"AUC\",\"mean\"), AP_mean=(\"AP\",\"mean\"), LL_mean=(\"LogLoss\",\"mean\"),\n",
    "         pos_valid=(\"pos_valid\",\"sum\"), n_valid=(\"n_valid\",\"sum\"))\n",
    ")\n",
    "\n",
    "summary_macro = (summary_per_model_target\n",
    "    .groupby(\"model\", as_index=False)\n",
    "    .agg(AUC_macro=(\"AUC_mean\",\"mean\"),\n",
    "         AP_macro=(\"AP_mean\",\"mean\"),\n",
    "         LL_macro=(\"LL_mean\",\"mean\"))\n",
    ")\n",
    "\n",
    "summary_rank = (rank_df\n",
    "    .groupby(\"model\", as_index=False)\n",
    "    .agg(users_eval=(\"users_eval\",\"sum\"),\n",
    "         Hit1=(\"Hit@1\",\"mean\"), Hit3=(\"Hit@3\",\"mean\"),\n",
    "         NDCG3=(\"NDCG@3\",\"mean\"), NDCG5=(\"NDCG@5\",\"mean\"))\n",
    ")\n",
    "\n",
    "print(\"\\n=== Macro (avg over targets) ===\")\n",
    "print(summary_macro.sort_values(\"AUC_macro\", ascending=False).round(4).to_string(index=False))\n",
    "\n",
    "print(\"\\n=== Ranking (avg over folds) ===\")\n",
    "print(summary_rank.sort_values(\"Hit3\", ascending=False).round(4).to_string(index=False))\n",
    "\n",
    "# Save\n",
    "metrics_df.to_csv(os.path.join(RESULTS_DIR, \"fold_metrics_per_target.csv\"), index=False)\n",
    "rank_df.to_csv(os.path.join(RESULTS_DIR, \"fold_metrics_ranking.csv\"), index=False)\n",
    "summary_per_model_target.to_csv(os.path.join(RESULTS_DIR, \"summary_per_model_target.csv\"), index=False)\n",
    "summary_macro.to_csv(os.path.join(RESULTS_DIR, \"summary_per_model_macro.csv\"), index=False)\n",
    "summary_rank.to_csv(os.path.join(RESULTS_DIR, \"summary_per_model_ranking.csv\"), index=False)\n",
    "\n",
    "print(\"\\nSaved results to:\", RESULTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d86b7c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data rows: 2127 | Targets: ['target_1', 'target_2', 'target_3', 'target_4']\n",
      "Fold counts: {0: 425, 1: 423, 2: 419, 3: 446, 4: 414}\n",
      "\n",
      "=== Fold 0 | train=1702 valid=425 ===\n",
      "\n",
      "=== Fold 1 | train=1704 valid=423 ===\n",
      "\n",
      "=== Fold 2 | train=1708 valid=419 ===\n",
      "\n",
      "=== Fold 3 | train=1681 valid=446 ===\n",
      "\n",
      "=== Fold 4 | train=1713 valid=414 ===\n",
      "\n",
      "=== Per-target (mean over folds) ===\n",
      "  target  AUC_mean  AP_mean  LL_mean  pos_valid  n_valid\n",
      "target_1    0.6973   0.4105   1.2415        556     2127\n",
      "target_2    0.7785   0.2134   0.1725         33     2127\n",
      "target_3    0.6912   0.2150   1.0337        276     2127\n",
      "target_4    0.7190   0.2952   0.9060        237     2127\n",
      "   MACRO    0.7215   0.2835   0.8384       1102     8508\n",
      "\n",
      "=== Ranking metrics (avg over folds) ===\n",
      " users_eval  Hit@1  Hit@3  NDCG@3  NDCG@5\n",
      "     1039.0 0.5515 0.9745  0.7981  0.8096\n",
      "Saved CV preds: /Users/tree/Projects/recommemdation_bank/outputs/predictions/lgbm/lgbm_cv_preds.parquet\n",
      "Saved Top-3 per user: /Users/tree/Projects/recommemdation_bank/outputs/predictions/lgbm/lgbm_cv_top3.jsonl\n",
      "\n",
      "Saved metrics to: /Users/tree/Projects/recommemdation_bank/outputs/metrics/lgbm\n"
     ]
    }
   ],
   "source": [
    "# 05_train_lgbm.py — LightGBM per-target with 5-fold CV on sparse TF-IDF features\n",
    "\n",
    "import os, json, gc, warnings, numpy as np, pandas as pd\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ====== CONFIG ======\n",
    "BASE_OUT   = \"/Users/tree/Projects/recommemdation_bank/outputs\"\n",
    "MM_JSONL   = f\"{BASE_OUT}/json/mm/json_balanced_mm.jsonl\"\n",
    "LABELS_PAR = f\"{BASE_OUT}/balanced/labels_mm_folded.parquet\"\n",
    "\n",
    "METRICS_DIR = f\"{BASE_OUT}/metrics/lgbm\"\n",
    "PRED_DIR    = f\"{BASE_OUT}/predictions/lgbm\"\n",
    "os.makedirs(METRICS_DIR, exist_ok=True)\n",
    "os.makedirs(PRED_DIR, exist_ok=True)\n",
    "\n",
    "FOLDS = [0,1,2,3,4]\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# TF-IDF settings (same spirit as your baseline)\n",
    "WORD_VECT = dict(\n",
    "    max_features=200_000,\n",
    "    ngram_range=(1,2),\n",
    "    min_df=2, max_df=0.995,\n",
    "    lowercase=True,\n",
    "    sublinear_tf=True,\n",
    "    preprocessor=lambda s: s.replace('.', '_')  # keep a6.24 → a6_24\n",
    ")\n",
    "CHAR_VECT = dict(\n",
    "    analyzer=\"char_wb\",\n",
    "    ngram_range=(3,5),\n",
    "    min_df=2, max_df=1.0,\n",
    "    lowercase=True\n",
    ")\n",
    "\n",
    "# LightGBM params\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "except Exception as e:\n",
    "    raise SystemExit(\n",
    "        f\"LightGBM not installed: {e}\\n\"\n",
    "        \"Install with one of:\\n\"\n",
    "        \"  pip install lightgbm\\n\"\n",
    "        \"  conda install -c conda-forge lightgbm\"\n",
    "    )\n",
    "\n",
    "LGBM_KW = dict(\n",
    "    objective=\"binary\",\n",
    "    boosting_type=\"gbdt\",\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=63,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.0,\n",
    "    reg_lambda=0.0,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# ====== helpers ======\n",
    "def load_mm_texts(path):\n",
    "    rows=[]\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            r = json.loads(line)\n",
    "            rows.append((str(r[\"client_id\"]), r[\"text\"]))\n",
    "    return pd.DataFrame(rows, columns=[\"client_id\",\"text\"])\n",
    "\n",
    "def build_sparse_features(train_text, valid_text):\n",
    "    vw = TfidfVectorizer(**WORD_VECT)\n",
    "    vc = TfidfVectorizer(**CHAR_VECT)\n",
    "    Xtr_w = vw.fit_transform(train_text); Xva_w = vw.transform(valid_text)\n",
    "    Xtr_c = vc.fit_transform(train_text); Xva_c = vc.transform(valid_text)\n",
    "    X_tr = hstack([Xtr_w, Xtr_c], format=\"csr\")\n",
    "    X_va = hstack([Xva_w, Xva_c], format=\"csr\")\n",
    "    return X_tr, X_va\n",
    "\n",
    "def class_weights(y):\n",
    "    pos = (y == 1).sum()\n",
    "    neg = (y == 0).sum()\n",
    "    if pos == 0 or neg == 0:\n",
    "        return None\n",
    "    w_pos = neg / max(pos, 1)\n",
    "    return np.where(y == 1, w_pos, 1.0)\n",
    "\n",
    "def safe_auc(y_true, y_prob):\n",
    "    y = np.asarray(y_true)\n",
    "    return np.nan if len(np.unique(y)) < 2 else roc_auc_score(y, y_prob)\n",
    "\n",
    "def safe_ap(y_true, y_prob):\n",
    "    y = np.asarray(y_true)\n",
    "    return np.nan if len(np.unique(y)) < 2 else average_precision_score(y, y_prob)\n",
    "\n",
    "def safe_logloss(y_true, y_prob):\n",
    "    y = np.asarray(y_true)\n",
    "    p = np.clip(y_prob, 1e-6, 1-1e-6)\n",
    "    return np.nan if len(np.unique(y)) < 2 else log_loss(y, p, labels=[0,1])\n",
    "\n",
    "def ndcg_at_k(y_true_row, y_score_row, k):\n",
    "    k = min(k, len(y_true_row))\n",
    "    if k <= 0: return np.nan\n",
    "    order = np.argsort(-y_score_row)\n",
    "    rel_k = np.take(y_true_row, order[:k])\n",
    "    discounts = 1.0 / np.log2(np.arange(2, k+2))\n",
    "    dcg  = np.sum((2**rel_k - 1) * discounts)\n",
    "    ideal = np.sort(y_true_row)[::-1][:k]\n",
    "    idcg = np.sum((2**ideal - 1) * discounts)\n",
    "    return np.nan if idcg == 0 else dcg / idcg\n",
    "\n",
    "def hit_at_k(y_true_row, y_score_row, k):\n",
    "    if y_true_row.sum() == 0: return np.nan\n",
    "    order = np.argsort(-y_score_row)[:k]\n",
    "    return 1.0 if y_true_row[order].sum() > 0 else 0.0\n",
    "\n",
    "# ====== load data ======\n",
    "texts  = load_mm_texts(MM_JSONL)\n",
    "labels = pd.read_parquet(LABELS_PAR)\n",
    "labels[\"client_id\"] = labels[\"client_id\"].astype(str)\n",
    "df = labels.merge(texts, on=\"client_id\", how=\"inner\")\n",
    "target_cols = [c for c in df.columns if c.startswith(\"target_\")]\n",
    "assert \"fold\" in df.columns\n",
    "\n",
    "print(\"Data rows:\", len(df), \"| Targets:\", target_cols)\n",
    "print(\"Fold counts:\", df[\"fold\"].value_counts().sort_index().to_dict())\n",
    "\n",
    "# ====== CV train/eval ======\n",
    "metric_rows = []\n",
    "rank_rows   = []\n",
    "pred_rows   = []   # per-user per-target probs from their own valid fold\n",
    "\n",
    "for fold in FOLDS:\n",
    "    train = df[df.fold != fold].reset_index(drop=True)\n",
    "    valid = df[df.fold == fold].reset_index(drop=True)\n",
    "    print(f\"\\n=== Fold {fold} | train={len(train)} valid={len(valid)} ===\")\n",
    "\n",
    "    X_tr, X_va = build_sparse_features(train[\"text\"], valid[\"text\"])\n",
    "    Y_tr = train[target_cols].values.astype(int)\n",
    "    Y_va = valid[target_cols].values.astype(int)\n",
    "\n",
    "    # One LGBM per target (one-vs-rest)\n",
    "    S_va = np.zeros_like(Y_va, dtype=float)\n",
    "    for ti, t in enumerate(target_cols):\n",
    "        y_tr = Y_tr[:, ti]\n",
    "        weights = class_weights(y_tr)\n",
    "\n",
    "        clf = lgb.LGBMClassifier(**LGBM_KW, verbosity=-1)\n",
    "        try:\n",
    "            clf.fit(X_tr, y_tr, sample_weight=weights)\n",
    "        except TypeError:\n",
    "            clf.fit(X_tr, y_tr)\n",
    "\n",
    "        pr = clf.predict_proba(X_va)[:, 1]\n",
    "        S_va[:, ti] = pr\n",
    "\n",
    "        metric_rows.append({\n",
    "            \"fold\": fold, \"target\": t,\n",
    "            \"AUC\": safe_auc(Y_va[:, ti], pr),\n",
    "            \"AP\":  safe_ap(Y_va[:, ti], pr),\n",
    "            \"LogLoss\": safe_logloss(Y_va[:, ti], pr),\n",
    "            \"pos_valid\": int(Y_va[:, ti].sum()), \"n_valid\": int(len(Y_va))\n",
    "        })\n",
    "\n",
    "    # save fold preds\n",
    "    fold_pred = pd.DataFrame(S_va, columns=[f\"{t}_prob\" for t in target_cols])\n",
    "    fold_pred.insert(0, \"client_id\", valid[\"client_id\"].values)\n",
    "    fold_pred.insert(1, \"fold\", int(fold))\n",
    "    pred_rows.append(fold_pred)\n",
    "\n",
    "    # ranking per user with ≥1 positive\n",
    "    hits1, hits3, ndcg3, ndcg5 = [], [], [], []\n",
    "    for i in range(len(valid)):\n",
    "        y_row, s_row = Y_va[i], S_va[i]\n",
    "        if y_row.sum() == 0: continue\n",
    "        hits1.append(hit_at_k(y_row, s_row, 1))\n",
    "        hits3.append(hit_at_k(y_row, s_row, 3))\n",
    "        ndcg3.append(ndcg_at_k(y_row, s_row, 3))\n",
    "        ndcg5.append(ndcg_at_k(y_row, s_row, 5))\n",
    "\n",
    "    rank_rows.append({\n",
    "        \"fold\": fold, \"users_eval\": int(len(hits1)),\n",
    "        \"Hit@1\": float(np.nanmean(hits1)) if hits1 else np.nan,\n",
    "        \"Hit@3\": float(np.nanmean(hits3)) if hits3 else np.nan,\n",
    "        \"NDCG@3\": float(np.nanmean(ndcg3)) if ndcg3 else np.nan,\n",
    "        \"NDCG@5\": float(np.nanmean(ndcg5)) if ndcg5 else np.nan,\n",
    "    })\n",
    "\n",
    "    del X_tr, X_va, Y_tr, Y_va\n",
    "    gc.collect()\n",
    "\n",
    "# ====== summarize & save ======\n",
    "metrics_df = pd.DataFrame(metric_rows)\n",
    "rank_df    = pd.DataFrame(rank_rows)\n",
    "cv_preds   = pd.concat(pred_rows, ignore_index=True)\n",
    "\n",
    "summary_target = (metrics_df\n",
    "                  .groupby(\"target\", as_index=False)\n",
    "                  .agg(AUC_mean=(\"AUC\",\"mean\"),\n",
    "                       AP_mean=(\"AP\",\"mean\"),\n",
    "                       LL_mean=(\"LogLoss\",\"mean\"),\n",
    "                       pos_valid=(\"pos_valid\",\"sum\"),\n",
    "                       n_valid=(\"n_valid\",\"sum\")))\n",
    "\n",
    "macro_row = {\n",
    "    \"target\": \"MACRO\",\n",
    "    \"AUC_mean\": summary_target[\"AUC_mean\"].mean(),\n",
    "    \"AP_mean\":  summary_target[\"AP_mean\"].mean(),\n",
    "    \"LL_mean\":  summary_target[\"LL_mean\"].mean(),\n",
    "    \"pos_valid\": int(summary_target[\"pos_valid\"].sum()),\n",
    "    \"n_valid\":   int(summary_target[\"n_valid\"].sum()),\n",
    "}\n",
    "summary_target = pd.concat([summary_target, pd.DataFrame([macro_row])], ignore_index=True)\n",
    "\n",
    "summary_rank = (rank_df\n",
    "                .agg({\"users_eval\":\"sum\",\n",
    "                      \"Hit@1\":\"mean\",\"Hit@3\":\"mean\",\n",
    "                      \"NDCG@3\":\"mean\",\"NDCG@5\":\"mean\"})\n",
    "                .to_frame(name=\"mean\").T)\n",
    "\n",
    "print(\"\\n=== Per-target (mean over folds) ===\")\n",
    "print(summary_target[[\"target\",\"AUC_mean\",\"AP_mean\",\"LL_mean\",\"pos_valid\",\"n_valid\"]]\n",
    "      .round(4).to_string(index=False))\n",
    "\n",
    "print(\"\\n=== Ranking metrics (avg over folds) ===\")\n",
    "print(summary_rank.round(4).to_string(index=False))\n",
    "\n",
    "# write metrics\n",
    "metrics_df.to_csv(os.path.join(METRICS_DIR, \"fold_metrics_per_target.csv\"), index=False)\n",
    "rank_df.to_csv(os.path.join(METRICS_DIR, \"fold_metrics_ranking.csv\"), index=False)\n",
    "summary_target.to_csv(os.path.join(METRICS_DIR, \"summary_per_target.csv\"), index=False)\n",
    "summary_rank.to_csv(os.path.join(METRICS_DIR, \"summary_ranking.csv\"), index=False)\n",
    "\n",
    "# write predictions + top-3\n",
    "cv_path = os.path.join(PRED_DIR, \"lgbm_cv_preds.parquet\")\n",
    "cv_preds.to_parquet(cv_path, index=False)\n",
    "print(\"Saved CV preds:\", cv_path)\n",
    "\n",
    "top3_path = os.path.join(PRED_DIR, \"lgbm_cv_top3.jsonl\")\n",
    "with open(top3_path, \"w\") as f:\n",
    "    for _, r in cv_preds.iterrows():\n",
    "        probs = [(c.replace(\"_prob\",\"\"), r[c]) for c in cv_preds.columns if c.endswith(\"_prob\")]\n",
    "        probs.sort(key=lambda x: -x[1])\n",
    "        top3 = [k for k,_ in probs[:3]]\n",
    "        f.write(json.dumps({\"client_id\": r[\"client_id\"], \"fold\": int(r[\"fold\"]), \"top3\": top3}) + \"\\n\")\n",
    "print(\"Saved Top-3 per user:\", top3_path)\n",
    "\n",
    "print(\"\\nSaved metrics to:\", METRICS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b75d25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data rows: 2127 | Targets: ['target_1', 'target_2', 'target_3', 'target_4']\n",
      "Fold counts: {0: 425, 1: 423, 2: 419, 3: 446, 4: 414}\n",
      "\n",
      "=== Fold 0 | train=1702 valid=425 ===\n",
      "\n",
      "=== Fold 1 | train=1704 valid=423 ===\n",
      "\n",
      "=== Fold 2 | train=1708 valid=419 ===\n",
      "\n",
      "=== Fold 3 | train=1681 valid=446 ===\n",
      "\n",
      "=== Fold 4 | train=1713 valid=414 ===\n",
      "Saved CV preds: /Users/tree/Projects/recommemdation_bank/outputs/predictions/svc_rbf/svc_cv_preds.parquet\n",
      "Saved Top-3 per user: /Users/tree/Projects/recommemdation_bank/outputs/predictions/svc_rbf/svc_cv_top3.jsonl\n"
     ]
    }
   ],
   "source": [
    "# 05_svc_oof.py — SVC (RBF) out-of-fold predictions on MM text\n",
    "\n",
    "import os, json, gc, numpy as np, pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.base import clone\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# ====== CONFIG ======\n",
    "BASE_OUT   = \"/Users/tree/Projects/recommemdation_bank/outputs\"\n",
    "MM_JSONL   = f\"{BASE_OUT}/json/mm/json_balanced_mm.jsonl\"\n",
    "LABELS_PAR = f\"{BASE_OUT}/balanced/labels_mm_folded.parquet\"\n",
    "\n",
    "PRED_DIR = f\"{BASE_OUT}/predictions/svc_rbf\"\n",
    "os.makedirs(PRED_DIR, exist_ok=True)\n",
    "\n",
    "FOLDS = [0,1,2,3,4]\n",
    "RANDOM_STATE = 42\n",
    "SVD_DIMS = 512\n",
    "\n",
    "# TF-IDF (same spirit as baseline)\n",
    "WORD_VECT = dict(\n",
    "    max_features=200_000, ngram_range=(1,2),\n",
    "    min_df=2, max_df=0.995, lowercase=True, sublinear_tf=True,\n",
    "    preprocessor=lambda s: s.replace('.', '_')   # keep tokens like a6.24 -> a6_24\n",
    ")\n",
    "CHAR_VECT = dict(\n",
    "    analyzer=\"char_wb\", ngram_range=(3,5),\n",
    "    min_df=2, max_df=1.0, lowercase=True\n",
    ")\n",
    "\n",
    "# ====== helpers ======\n",
    "def load_mm_texts(path):\n",
    "    rows=[]\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            r = json.loads(line)\n",
    "            rows.append((str(r[\"client_id\"]), r[\"text\"]))\n",
    "    return pd.DataFrame(rows, columns=[\"client_id\",\"text\"])\n",
    "\n",
    "def build_features(train_text, valid_text):\n",
    "    # Sparse TF-IDF (word + char)\n",
    "    vec_w = TfidfVectorizer(**WORD_VECT)\n",
    "    vec_c = TfidfVectorizer(**CHAR_VECT)\n",
    "    Xtr_w = vec_w.fit_transform(train_text); Xva_w = vec_w.transform(valid_text)\n",
    "    Xtr_c = vec_c.fit_transform(train_text); Xva_c = vec_c.transform(valid_text)\n",
    "    X_tr_sparse = hstack([Xtr_w, Xtr_c], format=\"csr\")\n",
    "    X_va_sparse = hstack([Xva_w, Xva_c], format=\"csr\")\n",
    "    # Dense projection for SVC\n",
    "    svd = TruncatedSVD(n_components=SVD_DIMS, random_state=RANDOM_STATE)\n",
    "    X_tr = svd.fit_transform(X_tr_sparse)\n",
    "    X_va = svd.transform(X_va_sparse)\n",
    "    scaler = StandardScaler(with_mean=True)\n",
    "    X_tr = scaler.fit_transform(X_tr)\n",
    "    X_va = scaler.transform(X_va)\n",
    "    return X_tr, X_va\n",
    "\n",
    "def class_weights(y):\n",
    "    pos = (y == 1).sum()\n",
    "    neg = (y == 0).sum()\n",
    "    if pos == 0 or neg == 0:\n",
    "        return None\n",
    "    w_pos = neg / max(pos, 1)\n",
    "    return np.where(y == 1, w_pos, 1.0)\n",
    "\n",
    "# ====== load ======\n",
    "texts  = load_mm_texts(MM_JSONL)\n",
    "labels = pd.read_parquet(LABELS_PAR)\n",
    "labels[\"client_id\"] = labels[\"client_id\"].astype(str)\n",
    "df = labels.merge(texts, on=\"client_id\", how=\"inner\")\n",
    "\n",
    "target_cols = [c for c in df.columns if c.startswith(\"target_\")]\n",
    "assert \"fold\" in df.columns\n",
    "print(\"Data rows:\", len(df), \"| Targets:\", target_cols)\n",
    "print(\"Fold counts:\", df[\"fold\"].value_counts().sort_index().to_dict())\n",
    "\n",
    "# ====== CV OOF preds ======\n",
    "pred_rows = []\n",
    "\n",
    "for fold in FOLDS:\n",
    "    train = df[df.fold != fold].reset_index(drop=True)\n",
    "    valid = df[df.fold == fold].reset_index(drop=True)\n",
    "    print(f\"\\n=== Fold {fold} | train={len(train)} valid={len(valid)} ===\")\n",
    "\n",
    "    X_tr, X_va = build_features(train[\"text\"], valid[\"text\"])\n",
    "    Y_tr = train[target_cols].values.astype(int)\n",
    "\n",
    "    S_va = np.zeros((len(valid), len(target_cols)), dtype=float)\n",
    "\n",
    "    for ti, t in enumerate(target_cols):\n",
    "        y_tr = Y_tr[:, ti]\n",
    "        weights = class_weights(y_tr)\n",
    "\n",
    "        clf = SVC(kernel=\"rbf\", probability=True, random_state=RANDOM_STATE)\n",
    "        try:\n",
    "            clf.fit(X_tr, y_tr, sample_weight=weights)\n",
    "        except TypeError:\n",
    "            clf.fit(X_tr, y_tr)\n",
    "\n",
    "        S_va[:, ti] = clf.predict_proba(X_va)[:, 1]\n",
    "\n",
    "    fold_pred = pd.DataFrame(S_va, columns=[f\"{t}_prob\" for t in target_cols])\n",
    "    fold_pred.insert(0, \"client_id\", valid[\"client_id\"].values)\n",
    "    fold_pred.insert(1, \"fold\", int(fold))\n",
    "    pred_rows.append(fold_pred)\n",
    "\n",
    "    del X_tr, X_va; gc.collect()\n",
    "\n",
    "cv_preds = pd.concat(pred_rows, ignore_index=True)\n",
    "\n",
    "# Save parquet\n",
    "cv_path = os.path.join(PRED_DIR, \"svc_cv_preds.parquet\")\n",
    "cv_preds.to_parquet(cv_path, index=False)\n",
    "print(\"Saved CV preds:\", cv_path)\n",
    "\n",
    "# Also save Top-3 per user (for quick inspection)\n",
    "top3_path = os.path.join(PRED_DIR, \"svc_cv_top3.jsonl\")\n",
    "with open(top3_path, \"w\") as f:\n",
    "    for _, r in cv_preds.iterrows():\n",
    "        probs = [(c.replace(\"_prob\",\"\"), r[c]) for c in cv_preds.columns if c.endswith(\"_prob\")]\n",
    "        probs.sort(key=lambda x: -x[1])\n",
    "        top3 = [k for k,_ in probs[:3]]\n",
    "        f.write(json.dumps({\"client_id\": r[\"client_id\"], \"fold\": int(r[\"fold\"]), \"top3\": top3}) + \"\\n\")\n",
    "print(\"Saved Top-3 per user:\", top3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be6b7ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data rows: 2127 | Targets: ['target_1', 'target_2', 'target_3', 'target_4']\n",
      "Fold counts: {0: 425, 1: 423, 2: 419, 3: 446, 4: 414}\n",
      "\n",
      "=== Fold 0 | train=1702 valid=425 ===\n",
      "\n",
      "=== Fold 1 | train=1704 valid=423 ===\n",
      "\n",
      "=== Fold 2 | train=1708 valid=419 ===\n",
      "\n",
      "=== Fold 3 | train=1681 valid=446 ===\n",
      "\n",
      "=== Fold 4 | train=1713 valid=414 ===\n",
      "Saved CV preds: /Users/tree/Projects/recommemdation_bank/outputs/predictions/lr/lr_cv_preds.parquet\n",
      "Saved Top-3 per user: /Users/tree/Projects/recommemdation_bank/outputs/predictions/lr/lr_cv_top3.jsonl\n"
     ]
    }
   ],
   "source": [
    "# 05_lr_oof.py — Logistic Regression out-of-fold predictions on MM text\n",
    "\n",
    "import os, json, gc, numpy as np, pandas as pd\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# ====== CONFIG ======\n",
    "BASE_OUT   = \"/Users/tree/Projects/recommemdation_bank/outputs\"\n",
    "MM_JSONL   = f\"{BASE_OUT}/json/mm/json_balanced_mm.jsonl\"\n",
    "LABELS_PAR = f\"{BASE_OUT}/balanced/labels_mm_folded.parquet\"\n",
    "\n",
    "PRED_DIR = f\"{BASE_OUT}/predictions/lr\"\n",
    "os.makedirs(PRED_DIR, exist_ok=True)\n",
    "\n",
    "FOLDS = [0,1,2,3,4]\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# TF-IDF (same spirit as your baseline/model zoo)\n",
    "WORD_VECT = dict(\n",
    "    max_features=200_000, ngram_range=(1,2),\n",
    "    min_df=2, max_df=0.995, lowercase=True, sublinear_tf=True,\n",
    "    preprocessor=lambda s: s.replace('.', '_')  # keep tokens like a6.24 -> a6_24\n",
    ")\n",
    "CHAR_VECT = dict(\n",
    "    analyzer=\"char_wb\", ngram_range=(3,5),\n",
    "    min_df=2, max_df=1.0, lowercase=True\n",
    ")\n",
    "\n",
    "LR_KW = dict(\n",
    "    solver=\"liblinear\",\n",
    "    C=1.0,\n",
    "    class_weight=\"balanced\",\n",
    "    max_iter=300,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# ====== helpers ======\n",
    "def load_mm_texts(path):\n",
    "    rows=[]\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            r = json.loads(line)\n",
    "            rows.append((str(r[\"client_id\"]), r[\"text\"]))\n",
    "    return pd.DataFrame(rows, columns=[\"client_id\",\"text\"])\n",
    "\n",
    "def build_sparse(train_text, valid_text):\n",
    "    vw = TfidfVectorizer(**WORD_VECT)\n",
    "    vc = TfidfVectorizer(**CHAR_VECT)\n",
    "    Xtr_w = vw.fit_transform(train_text); Xva_w = vw.transform(valid_text)\n",
    "    Xtr_c = vc.fit_transform(train_text); Xva_c = vc.transform(valid_text)\n",
    "    return hstack([Xtr_w, Xtr_c], format=\"csr\"), hstack([Xva_w, Xva_c], format=\"csr\")\n",
    "\n",
    "# ====== load ======\n",
    "texts  = load_mm_texts(MM_JSONL)\n",
    "labels = pd.read_parquet(LABELS_PAR)\n",
    "labels[\"client_id\"] = labels[\"client_id\"].astype(str)\n",
    "df = labels.merge(texts, on=\"client_id\", how=\"inner\")\n",
    "\n",
    "target_cols = [c for c in df.columns if c.startswith(\"target_\")]\n",
    "assert \"fold\" in df.columns, \"labels_mm_folded.parquet must contain a 'fold' column.\"\n",
    "\n",
    "print(\"Data rows:\", len(df), \"| Targets:\", target_cols)\n",
    "print(\"Fold counts:\", df[\"fold\"].value_counts().sort_index().to_dict())\n",
    "\n",
    "# ====== CV OOF preds ======\n",
    "pred_rows = []\n",
    "\n",
    "for fold in FOLDS:\n",
    "    train = df[df.fold != fold].reset_index(drop=True)\n",
    "    valid = df[df.fold == fold].reset_index(drop=True)\n",
    "    print(f\"\\n=== Fold {fold} | train={len(train)} valid={len(valid)} ===\")\n",
    "\n",
    "    X_tr, X_va = build_sparse(train[\"text\"], valid[\"text\"])\n",
    "\n",
    "    S_va = np.zeros((len(valid), len(target_cols)), dtype=float)\n",
    "    for ti, t in enumerate(target_cols):\n",
    "        y_tr = train[t].values.astype(int)\n",
    "        clf = LogisticRegression(**LR_KW)\n",
    "        clf.fit(X_tr, y_tr)\n",
    "        S_va[:, ti] = clf.predict_proba(X_va)[:, 1]\n",
    "\n",
    "    fold_pred = pd.DataFrame(S_va, columns=[f\"{t}_prob\" for t in target_cols])\n",
    "    fold_pred.insert(0, \"client_id\", valid[\"client_id\"].values)\n",
    "    fold_pred.insert(1, \"fold\", int(fold))\n",
    "    pred_rows.append(fold_pred)\n",
    "\n",
    "    del X_tr, X_va; gc.collect()\n",
    "\n",
    "cv_preds = pd.concat(pred_rows, ignore_index=True)\n",
    "\n",
    "# Save parquet (for stacking)\n",
    "cv_path = os.path.join(PRED_DIR, \"lr_cv_preds.parquet\")\n",
    "cv_preds.to_parquet(cv_path, index=False)\n",
    "print(\"Saved CV preds:\", cv_path)\n",
    "\n",
    "# Also save Top-3 per user (optional quick check)\n",
    "top3_path = os.path.join(PRED_DIR, \"lr_cv_top3.jsonl\")\n",
    "with open(top3_path, \"w\") as f:\n",
    "    for _, r in cv_preds.iterrows():\n",
    "        probs = [(c.replace(\"_prob\",\"\"), r[c]) for c in cv_preds.columns if c.endswith(\"_prob\")]\n",
    "        probs.sort(key=lambda x: -x[1])\n",
    "        top3 = [k for k,_ in probs[:3]]\n",
    "        f.write(json.dumps({\"client_id\": r[\"client_id\"], \"fold\": int(r[\"fold\"]), \"top3\": top3}) + \"\\n\")\n",
    "print(\"Saved Top-3 per user:\", top3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "460d8b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Found LR CV preds: /Users/tree/Projects/recommemdation_bank/outputs/predictions/lr/lr_cv_preds.parquet\n",
      "[INFO] Using models in stack: ['LGBM', 'LR', 'SVC']\n",
      "\n",
      "=== Stacked (LGBM + LR + SVC) — per-target mean over folds ===\n",
      "  target  AUC_mean  AP_mean  LL_mean  pos_valid  n_valid\n",
      "target_1    0.6954   0.4139   0.6396        556     2127\n",
      "target_2    0.7728   0.3450   0.5359         33     2127\n",
      "target_3    0.6840   0.2280   0.6465        276     2127\n",
      "target_4    0.7570   0.3789   0.5746        237     2127\n",
      "   MACRO    0.7273   0.3414   0.5992       1102     8508\n",
      "\n",
      "=== Stacked — ranking (avg over folds) ===\n",
      " users_eval   Hit1   Hit3  NDCG3  NDCG5\n",
      "     1039.0 0.4778 0.9105  0.726 0.7687\n",
      "Saved stacked CV preds: /Users/tree/Projects/recommemdation_bank/outputs/predictions/stack/stack_cv_preds.parquet\n",
      "Saved Top-3 per user: /Users/tree/Projects/recommemdation_bank/outputs/predictions/stack/stack_cv_top3.jsonl\n"
     ]
    }
   ],
   "source": [
    "# 05_stack_meta.py — OOF stacking of LGBM + LR (+ SVC if present)\n",
    "\n",
    "import os, json, gc, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "BASE_OUT = \"/Users/tree/Projects/recommemdation_bank/outputs\"\n",
    "LABELS_PAR = f\"{BASE_OUT}/balanced/labels_mm_folded.parquet\"\n",
    "MM_JSONL   = f\"{BASE_OUT}/json/mm/json_balanced_mm.jsonl\"\n",
    "P_LGBM = f\"{BASE_OUT}/predictions/lgbm/lgbm_cv_preds.parquet\"\n",
    "P_LR   = f\"{BASE_OUT}/predictions/lr/lr_cv_preds.parquet\"\n",
    "P_SVC  = f\"{BASE_OUT}/predictions/svc_rbf/svc_cv_preds.parquet\"\n",
    "METRICS_DIR = f\"{BASE_OUT}/metrics/stack\"; PRED_DIR = f\"{BASE_OUT}/predictions/stack\"\n",
    "os.makedirs(METRICS_DIR, exist_ok=True); os.makedirs(PRED_DIR, exist_ok=True)\n",
    "FOLDS=[0,1,2,3,4]; RANDOM_STATE=42\n",
    "\n",
    "def safe_auc(y,p): \n",
    "    y=np.asarray(y); \n",
    "    return np.nan if len(np.unique(y))<2 else roc_auc_score(y,p)\n",
    "def safe_ap(y,p):  \n",
    "    y=np.asarray(y); \n",
    "    return np.nan if len(np.unique(y))<2 else average_precision_score(y,p)\n",
    "def safe_ll(y,p):  \n",
    "    y=np.asarray(y); \n",
    "    p=np.clip(p,1e-6,1-1e-6); \n",
    "    return np.nan if len(np.unique(y))<2 else log_loss(y,p,labels=[0,1])\n",
    "def ndcg_at_k(y_true_row, y_score_row, k):\n",
    "    k=min(k, len(y_true_row)); \n",
    "    if k<=0: return np.nan\n",
    "    order=np.argsort(-y_score_row); rel=np.take(y_true_row, order[:k])\n",
    "    disc=1.0/np.log2(np.arange(2,k+2)); \n",
    "    dcg=np.sum((2**rel-1)*disc); ideal=np.sort(y_true_row)[::-1][:k]\n",
    "    idcg=np.sum((2**ideal-1)*disc); \n",
    "    return np.nan if idcg==0 else dcg/idcg\n",
    "def hit_at_k(y_true_row, y_score_row, k):\n",
    "    if y_true_row.sum()==0: return np.nan\n",
    "    order=np.argsort(-y_score_row)[:k]; \n",
    "    return 1.0 if y_true_row[order].sum()>0 else 0.0\n",
    "def hit_ndcg_block(Y,S):\n",
    "    H1,H3,N3,N5=[],[],[],[]\n",
    "    for i in range(Y.shape[0]):\n",
    "        if Y[i].sum()==0: continue\n",
    "        H1.append(hit_at_k(Y[i],S[i],1)); H3.append(hit_at_k(Y[i],S[i],3))\n",
    "        N3.append(ndcg_at_k(Y[i],S[i],3)); N5.append(ndcg_at_k(Y[i],S[i],5))\n",
    "    return dict(users_eval=len(H1), Hit1=float(np.nanmean(H1)) if H1 else np.nan,\n",
    "                Hit3=float(np.nanmean(H3)) if H3 else np.nan,\n",
    "                NDCG3=float(np.nanmean(N3)) if N3 else np.nan,\n",
    "                NDCG5=float(np.nanmean(N5)) if N5 else np.nan)\n",
    "\n",
    "def ensure_lr_preds():\n",
    "    if os.path.exists(P_LR): \n",
    "        print(\"[OK] Found LR CV preds:\", P_LR); \n",
    "        return\n",
    "    print(\"[MAKE] LR CV preds not found — training LR OOF now…\")\n",
    "    rows=[]; \n",
    "    with open(MM_JSONL) as f:\n",
    "        for line in f:\n",
    "            r=json.loads(line); rows.append((str(r[\"client_id\"]), r[\"text\"]))\n",
    "    texts = pd.DataFrame(rows, columns=[\"client_id\",\"text\"])\n",
    "    labels = pd.read_parquet(LABELS_PAR); labels[\"client_id\"]=labels[\"client_id\"].astype(str)\n",
    "    df = labels.merge(texts, on=\"client_id\", how=\"inner\")\n",
    "    target_cols = [c for c in df.columns if c.startswith(\"target_\")]\n",
    "\n",
    "    WORD_VECT = dict(max_features=200_000, ngram_range=(1,2), min_df=2, max_df=0.995,\n",
    "                     lowercase=True, sublinear_tf=True, preprocessor=lambda s: s.replace('.', '_'))\n",
    "    CHAR_VECT = dict(analyzer=\"char_wb\", ngram_range=(3,5), min_df=2, max_df=1.0, lowercase=True)\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from scipy.sparse import hstack\n",
    "    LR_KW = dict(solver=\"liblinear\", C=1.0, class_weight=\"balanced\", max_iter=300, random_state=RANDOM_STATE)\n",
    "\n",
    "    def build_sparse(tr_text, va_text):\n",
    "        vw=TfidfVectorizer(**WORD_VECT); vc=TfidfVectorizer(**CHAR_VECT)\n",
    "        Xtr_w=vw.fit_transform(tr_text); Xva_w=vw.transform(va_text)\n",
    "        Xtr_c=vc.fit_transform(tr_text); Xva_c=vc.transform(va_text)\n",
    "        return hstack([Xtr_w,Xtr_c],format=\"csr\"), hstack([Xva_w,Xva_c],format=\"csr\")\n",
    "\n",
    "    pred_rows=[]\n",
    "    for fold in FOLDS:\n",
    "        tr=df[df.fold!=fold].reset_index(drop=True); va=df[df.fold==fold].reset_index(drop=True)\n",
    "        Xtr,Xva = build_sparse(tr[\"text\"], va[\"text\"])\n",
    "        S = np.zeros((len(va), len(target_cols)))\n",
    "        for ti,t in enumerate(target_cols):\n",
    "            y = tr[t].values.astype(int)\n",
    "            clf = LogisticRegression(**LR_KW).fit(Xtr, y)\n",
    "            S[:,ti] = clf.predict_proba(Xva)[:,1]\n",
    "        out = pd.DataFrame(S, columns=[f\"{t}_prob\" for t in target_cols])\n",
    "        out.insert(0,\"client_id\", va[\"client_id\"].values)\n",
    "        out.insert(1,\"fold\", int(fold))\n",
    "        pred_rows.append(out)\n",
    "        del Xtr,Xva; gc.collect()\n",
    "\n",
    "    cv_lr = pd.concat(pred_rows, ignore_index=True)\n",
    "    os.makedirs(os.path.dirname(P_LR), exist_ok=True)\n",
    "    cv_lr.to_parquet(P_LR, index=False)\n",
    "    print(\"[SAVED] LR CV preds ->\", P_LR)\n",
    "\n",
    "# prerequisites\n",
    "assert os.path.exists(P_LGBM), f\"LGBM preds not found at {P_LGBM}. Run the LGBM cell first.\"\n",
    "ensure_lr_preds()\n",
    "use_svc = os.path.exists(P_SVC)\n",
    "print(\"[INFO] Using models in stack:\", [\"LGBM\",\"LR\"] + ([\"SVC\"] if use_svc else []))\n",
    "\n",
    "# load OOF preds\n",
    "lgbm = pd.read_parquet(P_LGBM)\n",
    "lr   = pd.read_parquet(P_LR)\n",
    "if use_svc: svc = pd.read_parquet(P_SVC)\n",
    "\n",
    "# rename with suffixes and join\n",
    "def add_suffix(df, suffix):\n",
    "    df = df.copy()\n",
    "    for c in list(df.columns):\n",
    "        if c.endswith(\"_prob\"):\n",
    "            df[c.replace(\"_prob\",\"\")+f\"_prob_{suffix}\"] = df[c]\n",
    "            del df[c]\n",
    "    return df\n",
    "lgbm = add_suffix(lgbm, \"lgbm\"); lr = add_suffix(lr, \"lr\")\n",
    "base = lgbm.merge(lr, on=[\"client_id\",\"fold\"], how=\"inner\")\n",
    "if use_svc:\n",
    "    svc = add_suffix(svc, \"svc\")\n",
    "    base = base.merge(svc, on=[\"client_id\",\"fold\"], how=\"inner\")\n",
    "\n",
    "# labels\n",
    "labels = pd.read_parquet(LABELS_PAR); labels[\"client_id\"]=labels[\"client_id\"].astype(str)\n",
    "targets = [c for c in labels.columns if c.startswith(\"target_\")]\n",
    "base = base.merge(labels[[\"client_id\",\"fold\"]+targets], on=[\"client_id\",\"fold\"], how=\"left\")\n",
    "\n",
    "# meta-learning OOF\n",
    "meta_rows=[]; rank_rows=[]; pred_rows=[]\n",
    "for fold in FOLDS:\n",
    "    tr = base[base.fold != fold].reset_index(drop=True)\n",
    "    va = base[base.fold == fold].reset_index(drop=True)\n",
    "    Y_va = va[targets].values.astype(int)\n",
    "\n",
    "    def feat_cols_for(t):\n",
    "        cols=[f\"{t}_prob_lgbm\", f\"{t}_prob_lr\"]\n",
    "        if use_svc: cols.append(f\"{t}_prob_svc\")\n",
    "        return cols\n",
    "\n",
    "    S_va = np.zeros((len(va), len(targets)), dtype=float)\n",
    "    for ti,t in enumerate(targets):\n",
    "        Xtr = tr[feat_cols_for(t)].values; ytr = tr[t].values.astype(int)\n",
    "        Xva = va[feat_cols_for(t)].values\n",
    "        meta = LogisticRegression(solver=\"liblinear\", class_weight=\"balanced\", random_state=RANDOM_STATE)\n",
    "        meta.fit(Xtr, ytr)\n",
    "        p = meta.predict_proba(Xva)[:,1]\n",
    "        S_va[:, ti] = p\n",
    "        meta_rows.append({\"fold\":fold,\"target\":t,\"AUC\":safe_auc(Y_va[:, ti], p),\n",
    "                          \"AP\":safe_ap(Y_va[:, ti], p),\"LogLoss\":safe_ll(Y_va[:, ti], p),\n",
    "                          \"pos_valid\":int(Y_va[:, ti].sum()),\"n_valid\":int(len(Y_va))})\n",
    "\n",
    "    fold_pred = pd.DataFrame(S_va, columns=[f\"{t}_prob_stack\" for t in targets])\n",
    "    fold_pred.insert(0,\"client_id\", va[\"client_id\"].values); fold_pred.insert(1,\"fold\", int(fold))\n",
    "    pred_rows.append(fold_pred)\n",
    "\n",
    "    rk = hit_ndcg_block(Y_va, S_va); rk[\"fold\"]=fold; rank_rows.append(rk)\n",
    "\n",
    "# summarize + save\n",
    "met = pd.DataFrame(meta_rows)\n",
    "summary_target = (met.groupby(\"target\", as_index=False)\n",
    "                    .agg(AUC_mean=(\"AUC\",\"mean\"), AP_mean=(\"AP\",\"mean\"), LL_mean=(\"LogLoss\",\"mean\"),\n",
    "                         pos_valid=(\"pos_valid\",\"sum\"), n_valid=(\"n_valid\",\"sum\")))\n",
    "macro = {\"target\":\"MACRO\",\"AUC_mean\":summary_target[\"AUC_mean\"].mean(),\n",
    "         \"AP_mean\":summary_target[\"AP_mean\"].mean(),\"LL_mean\":summary_target[\"LL_mean\"].mean(),\n",
    "         \"pos_valid\":int(summary_target[\"pos_valid\"].sum()),\"n_valid\":int(summary_target[\"n_valid\"].sum())}\n",
    "summary_target = pd.concat([summary_target, pd.DataFrame([macro])], ignore_index=True)\n",
    "\n",
    "rkdf = pd.DataFrame(rank_rows)\n",
    "summary_rank = (rkdf.agg({\"users_eval\":\"sum\",\"Hit1\":\"mean\",\"Hit3\":\"mean\",\"NDCG3\":\"mean\",\"NDCG5\":\"mean\"})\n",
    "                .to_frame(\"mean\").T)\n",
    "\n",
    "print(\"\\n=== Stacked (LGBM + LR\" + (\" + SVC\" if use_svc else \"\") + \") — per-target mean over folds ===\")\n",
    "print(summary_target[[\"target\",\"AUC_mean\",\"AP_mean\",\"LL_mean\",\"pos_valid\",\"n_valid\"]]\n",
    "      .round(4).to_string(index=False))\n",
    "print(\"\\n=== Stacked — ranking (avg over folds) ===\")\n",
    "print(summary_rank.round(4).to_string(index=False))\n",
    "\n",
    "met.to_csv(os.path.join(METRICS_DIR, \"stack_fold_metrics_per_target.csv\"), index=False)\n",
    "summary_target.to_csv(os.path.join(METRICS_DIR, \"stack_summary_per_target.csv\"), index=False)\n",
    "summary_rank.to_csv(os.path.join(METRICS_DIR, \"stack_summary_ranking.csv\"), index=False)\n",
    "\n",
    "cv_stack = pd.concat(pred_rows, ignore_index=True)\n",
    "cv_path = os.path.join(PRED_DIR, \"stack_cv_preds.parquet\")\n",
    "cv_stack.to_parquet(cv_path, index=False)\n",
    "print(\"Saved stacked CV preds:\", cv_path)\n",
    "\n",
    "top3_path = os.path.join(PRED_DIR, \"stack_cv_top3.jsonl\")\n",
    "with open(top3_path, \"w\") as f:\n",
    "    for _, r in cv_stack.iterrows():\n",
    "        probs = [(c.replace(\"_prob_stack\",\"\"), r[c]) for c in cv_stack.columns if c.endswith(\"_prob_stack\")]\n",
    "        probs.sort(key=lambda x: -x[1])\n",
    "        f.write(json.dumps({\"client_id\": r[\"client_id\"], \"fold\": int(r[\"fold\"]), \"top3\": [k for k,_ in probs[:3]]}) + \"\\n\")\n",
    "print(\"Saved Top-3 per user:\", top3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adfd9ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best weights (by NDCG@3, tie=Hit@1): w_svc=0.10, w_lgbm=0.90, w_lr=0.00\n",
      "\n",
      "=== Blended ranking (avg over folds) ===\n",
      " users_eval  Hit@1  Hit@3  NDCG@3  NDCG@5\n",
      "     1039.0 0.5752 0.9809  0.8124  0.8214\n",
      "Saved blended CV preds: /Users/tree/Projects/recommemdation_bank/outputs/predictions/blend/blend_cv_preds.parquet\n",
      "Saved Top-3 per user: /Users/tree/Projects/recommemdation_bank/outputs/predictions/blend/blend_cv_top3.jsonl\n"
     ]
    }
   ],
   "source": [
    "# 05_blend_meta.py — grid-searched weight blend of SVC + LGBM + LR to optimize NDCG@3\n",
    "\n",
    "import os, json, numpy as np, pandas as pd\n",
    "\n",
    "BASE_OUT = \"/Users/tree/Projects/recommemdation_bank/outputs\"\n",
    "LABELS_PAR = f\"{BASE_OUT}/balanced/labels_mm_folded.parquet\"\n",
    "P_SVC  = f\"{BASE_OUT}/predictions/svc_rbf/svc_cv_preds.parquet\"\n",
    "P_LGBM = f\"{BASE_OUT}/predictions/lgbm/lgbm_cv_preds.parquet\"\n",
    "P_LR   = f\"{BASE_OUT}/predictions/lr/lr_cv_preds.parquet\"\n",
    "\n",
    "OUT_DIR = f\"{BASE_OUT}/predictions/blend\"\n",
    "MET_DIR = f\"{BASE_OUT}/metrics/blend\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "os.makedirs(MET_DIR, exist_ok=True)\n",
    "\n",
    "FOLDS = [0,1,2,3,4]\n",
    "\n",
    "# --- helpers ---\n",
    "def ndcg_at_k(y_true_row, y_score_row, k):\n",
    "    k = min(k, len(y_true_row))\n",
    "    if k <= 0: return np.nan\n",
    "    order = np.argsort(-y_score_row)\n",
    "    rel_k = np.take(y_true_row, order[:k])\n",
    "    discounts = 1.0/np.log2(np.arange(2, k+2))\n",
    "    dcg = np.sum((2**rel_k - 1) * discounts)\n",
    "    ideal = np.sort(y_true_row)[::-1][:k]\n",
    "    idcg = np.sum((2**ideal - 1) * discounts)\n",
    "    return np.nan if idcg == 0 else dcg/idcg\n",
    "\n",
    "def hit_at_k(y_true_row, y_score_row, k):\n",
    "    if y_true_row.sum() == 0: return np.nan\n",
    "    order = np.argsort(-y_score_row)[:k]\n",
    "    return 1.0 if y_true_row[order].sum() > 0 else 0.0\n",
    "\n",
    "def rank_metrics(Y, S):\n",
    "    H1,H3,N3,N5=[],[],[],[]\n",
    "    for i in range(Y.shape[0]):\n",
    "        if Y[i].sum()==0: continue\n",
    "        H1.append(hit_at_k(Y[i], S[i], 1))\n",
    "        H3.append(hit_at_k(Y[i], S[i], 3))\n",
    "        N3.append(ndcg_at_k(Y[i], S[i], 3))\n",
    "        N5.append(ndcg_at_k(Y[i], S[i], 5))\n",
    "    return {\n",
    "        \"users_eval\": len(H1),\n",
    "        \"Hit@1\": float(np.nanmean(H1)) if H1 else np.nan,\n",
    "        \"Hit@3\": float(np.nanmean(H3)) if H3 else np.nan,\n",
    "        \"NDCG@3\": float(np.nanmean(N3)) if N3 else np.nan,\n",
    "        \"NDCG@5\": float(np.nanmean(N5)) if N5 else np.nan,\n",
    "    }\n",
    "\n",
    "# --- load labels & preds ---\n",
    "labels = pd.read_parquet(LABELS_PAR)\n",
    "labels[\"client_id\"] = labels[\"client_id\"].astype(str)\n",
    "targets = [c for c in labels.columns if c.startswith(\"target_\")]\n",
    "\n",
    "svc  = pd.read_parquet(P_SVC)\n",
    "lgbm = pd.read_parquet(P_LGBM)\n",
    "lr   = pd.read_parquet(P_LR)\n",
    "\n",
    "# align and stack into dict[fold] -> (Y, S_model)\n",
    "def fold_pack(df, model_suffix):\n",
    "    cols = [c for c in df.columns if c.endswith(\"_prob\")]\n",
    "    packed = {}\n",
    "    for f in FOLDS:\n",
    "        d = df[df.fold==f].merge(labels[[\"client_id\",\"fold\"]+targets], on=[\"client_id\",\"fold\"], how=\"left\")\n",
    "        Y = d[targets].values.astype(int)\n",
    "        S = d[[c for c in cols]].values.astype(float)\n",
    "        # ensure same order of targets for all models\n",
    "        order = [f\"{t}_prob\" for t in targets]\n",
    "        S = d[order].values.astype(float)\n",
    "        packed[f] = (Y, S)\n",
    "    return packed\n",
    "\n",
    "P_svc  = fold_pack(svc,  \"svc\")\n",
    "P_lgbm = fold_pack(lgbm, \"lgbm\")\n",
    "P_lr   = fold_pack(lr,   \"lr\")\n",
    "\n",
    "# --- grid search weights (w_svc, w_lgbm, w_lr >=0, sum=1) to maximize mean NDCG@3 ---\n",
    "best = None\n",
    "grid = np.linspace(0, 1, 11)  # step 0.1; small & fast\n",
    "for ws in grid:\n",
    "    for wl in grid:\n",
    "        wr = 1.0 - ws - wl\n",
    "        if wr < 0 or wr > 1: \n",
    "            continue\n",
    "        # aggregate metrics across folds\n",
    "        mets = []\n",
    "        for f in FOLDS:\n",
    "            Y,_ = P_svc[f]\n",
    "            S = ws*P_svc[f][1] + wl*P_lgbm[f][1] + wr*P_lr[f][1]\n",
    "            mets.append(rank_metrics(Y, S))\n",
    "        # average\n",
    "        N3 = np.mean([m[\"NDCG@3\"] for m in mets])\n",
    "        H1 = np.mean([m[\"Hit@1\"] for m in mets])\n",
    "        # pick by NDCG@3, break ties with Hit@1\n",
    "        score = (N3, H1)\n",
    "        if (best is None) or (score > best[0]):\n",
    "            best = (score, (ws, wl, wr))\n",
    "\n",
    "(ws, wl, wr) = best[1]\n",
    "print(f\"Best weights (by NDCG@3, tie=Hit@1): w_svc={ws:.2f}, w_lgbm={wl:.2f}, w_lr={wr:.2f}\")\n",
    "\n",
    "# --- evaluate & save blended OOF ---\n",
    "rank_rows = []\n",
    "pred_rows = []\n",
    "for f in FOLDS:\n",
    "    Y,_ = P_svc[f]\n",
    "    S = ws*P_svc[f][1] + wl*P_lgbm[f][1] + wr*P_lr[f][1]\n",
    "    rk = rank_metrics(Y, S); rk[\"fold\"]=f; rank_rows.append(rk)\n",
    "    # save fold preds with columns target_i_prob_blend\n",
    "    d = svc[svc.fold==f][[\"client_id\",\"fold\"]].copy()\n",
    "    for i,t in enumerate(targets):\n",
    "        d[f\"{t}_prob_blend\"] = S[:, i]\n",
    "    pred_rows.append(d)\n",
    "\n",
    "rkdf = pd.DataFrame(rank_rows)\n",
    "summary_rank = rkdf.agg({\"users_eval\":\"sum\",\"Hit@1\":\"mean\",\"Hit@3\":\"mean\",\"NDCG@3\":\"mean\",\"NDCG@5\":\"mean\"}).to_frame(\"mean\").T\n",
    "print(\"\\n=== Blended ranking (avg over folds) ===\")\n",
    "print(summary_rank.round(4).to_string(index=False))\n",
    "\n",
    "cv_blend = pd.concat(pred_rows, ignore_index=True)\n",
    "cv_path = os.path.join(OUT_DIR, \"blend_cv_preds.parquet\")\n",
    "cv_blend.to_parquet(cv_path, index=False)\n",
    "print(\"Saved blended CV preds:\", cv_path)\n",
    "\n",
    "# Top-3 per user\n",
    "top3_path = os.path.join(OUT_DIR, \"blend_cv_top3.jsonl\")\n",
    "with open(top3_path, \"w\") as f:\n",
    "    for _, r in cv_blend.iterrows():\n",
    "        probs = [(c.replace(\"_prob_blend\",\"\"), r[c]) for c in cv_blend.columns if c.endswith(\"_prob_blend\")]\n",
    "        probs.sort(key=lambda x: -x[1])\n",
    "        f.write(json.dumps({\"client_id\": r[\"client_id\"], \"fold\": int(r[\"fold\"]), \"top3\": [k for k,_ in probs[:3]]}) + \"\\n\")\n",
    "print(\"Saved Top-3 per user:\", top3_path)\n",
    "\n",
    "# also write weights & metrics\n",
    "pd.DataFrame([{\"w_svc\":ws, \"w_lgbm\":wl, \"w_lr\":wr}]).to_csv(os.path.join(MET_DIR,\"blend_weights.csv\"), index=False)\n",
    "summary_rank.to_csv(os.path.join(MET_DIR,\"blend_summary_ranking.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
